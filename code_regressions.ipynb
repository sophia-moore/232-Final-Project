{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophia-moore/232-Final-Project/blob/main/code_regressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ec1435",
      "metadata": {
        "id": "75ec1435"
      },
      "source": [
        "# Predictive Regressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990d55c9",
      "metadata": {
        "id": "990d55c9"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Sequence, Dict, Tuple, Optional\n",
        "from collections import Counter\n",
        "from typing import Tuple, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fedf0f",
      "metadata": {
        "id": "49fedf0f"
      },
      "outputs": [],
      "source": [
        "def read_csv_to_df(file_path, index_col=None):\n",
        "\t\"\"\"\n",
        "\tRead a CSV file and convert it to a pandas DataFrame\n",
        "\n",
        "\tArgs:\n",
        "\t\tfile_path (str): Path to the CSV file\n",
        "\n",
        "\tReturns:\n",
        "\t\tpandas.DataFrame: The loaded DataFrame\n",
        "\t\"\"\"\n",
        "\tdf = pd.read_csv(file_path, index_col=index_col)\n",
        "\treturn df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00a9475",
      "metadata": {
        "id": "c00a9475",
        "outputId": "3eaf1dbc-0a7b-48f5-af02-b39e17d4b223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "printing SIMPLE AVG: (8378, 10) (2793, 10) (8378, 10) (2793, 10) (8378, 10) (2793, 10)\n",
            "printing TFIDF: (8378, 10) (2793, 10) (8378, 10) (2793, 10) (8378, 10) (2793, 10)\n"
          ]
        }
      ],
      "source": [
        "# SIMPLE AVG\n",
        "Z_train_simple_pca = read_csv_to_df('Z_train_pca_glove_avg.csv', index_col=0)\n",
        "Z_test_simple_pca = read_csv_to_df('Z_test_pca_glove_avg.csv', index_col=0)\n",
        "\n",
        "Z_train_simple_lap = read_csv_to_df('Z_train_lap_avg.csv')\n",
        "Z_test_simple_lap = read_csv_to_df('Z_test_lap_avg.csv')\n",
        "\n",
        "Z_train_simple_diff = read_csv_to_df('Z_train_diff_t2_avg.csv', index_col=0)\n",
        "Z_test_simple_diff = read_csv_to_df('Z_test_diff_t2_avg.csv', index_col=0)\n",
        "\n",
        "print(\"printing SIMPLE AVG:\", Z_train_simple_pca.shape, Z_test_simple_pca.shape, Z_train_simple_lap.shape, Z_test_simple_lap.shape, Z_train_simple_diff.shape, Z_test_simple_diff.shape)\n",
        "\n",
        "# TFIDF\n",
        "Z_train_tfidf_pca = read_csv_to_df('Z_train_pca_glove_tfidf.csv', index_col=0)\n",
        "Z_test_tfidf_pca = read_csv_to_df('Z_test_pca_glove_tfidf.csv', index_col=0)\n",
        "\n",
        "Z_train_tfidf_lap = read_csv_to_df('Z_train_lap_tfidf.csv')\n",
        "Z_test_tfidf_lap = read_csv_to_df('Z_test_lap_tfidf.csv')\n",
        "\n",
        "Z_train_tfidf_diff = read_csv_to_df('Z_train_diff_t2_tfidf.csv', index_col=0)\n",
        "Z_test_tfidf_diff = read_csv_to_df('Z_test_diff_t2_tfidf.csv', index_col=0)\n",
        "\n",
        "print(\"printing TFIDF:\", Z_train_tfidf_pca.shape, Z_test_tfidf_pca.shape, Z_train_tfidf_lap.shape, Z_test_tfidf_lap.shape, Z_train_tfidf_diff.shape, Z_test_tfidf_diff.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8e3b91",
      "metadata": {
        "id": "bf8e3b91",
        "outputId": "91bce7d3-8b2e-44eb-d713-83e4f974ab84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "0",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "1",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "2",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "3",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "4",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "5",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "6",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "7",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "8",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "9",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "55b1e54d-c842-401b-9130-0c225d2ee175",
              "rows": [
                [
                  "0",
                  "0.0088013481173662",
                  "-0.0072354204086162",
                  "0.0057164761192578",
                  "-0.0221180729185407",
                  "-0.0004506521500109",
                  "0.0197777737495923",
                  "0.0116767801122764",
                  "0.0067888652778132",
                  "-0.007946704780221",
                  "0.0156742160913018"
                ],
                [
                  "1",
                  "0.0114680281021546",
                  "0.0064590769792283",
                  "-6.221899170791308e-06",
                  "-0.0047699772309387",
                  "-0.002964207455183",
                  "-0.0240449667499658",
                  "-0.0003028072712331",
                  "0.0048722497392955",
                  "-0.0017889500992081",
                  "-0.0057174577704961"
                ],
                [
                  "2",
                  "-0.0001405997207629",
                  "-0.0126239189814783",
                  "0.0172703502755822",
                  "-0.001190135849885",
                  "-0.0008065993854946",
                  "0.013633122421291",
                  "0.0138789169301487",
                  "-0.0049417406949527",
                  "0.0105452979800682",
                  "0.00937425737199"
                ],
                [
                  "3",
                  "-0.0027342616335118",
                  "0.007074811693875",
                  "-0.0014941634171615",
                  "-0.021818988485368",
                  "-0.0057813458010294",
                  "0.0086955059357615",
                  "-0.0009118823823635",
                  "0.0009774059364542",
                  "0.0089809001749364",
                  "-0.0096957213384801"
                ],
                [
                  "4",
                  "-0.0166539498554634",
                  "0.0110777413220623",
                  "0.0106597339091703",
                  "0.0062915713170971",
                  "-0.0098474118977017",
                  "0.0010055821916289",
                  "-0.0018300300648902",
                  "0.0053909423278421",
                  "0.0043597691714601",
                  "-0.0100227132320769"
                ],
                [
                  "5",
                  "-0.0043480385286019",
                  "-0.0132293173260783",
                  "0.0117975173058352",
                  "0.0098238596459174",
                  "-0.003067984044629",
                  "-0.0069860142158397",
                  "0.0076415553813777",
                  "0.000892963488406",
                  "-0.0070431805609297",
                  "-0.0064245565086751"
                ],
                [
                  "6",
                  "0.0051025539708061",
                  "-0.005451199111401",
                  "-0.0018482117179965",
                  "0.0185252163437853",
                  "0.0084150872806874",
                  "0.008715195458077",
                  "-0.0213627532017614",
                  "0.001045330021385",
                  "0.0059800200167039",
                  "-0.0058549655853198"
                ],
                [
                  "7",
                  "0.0190753624702205",
                  "0.0105311850010945",
                  "-0.010643680516737",
                  "0.0146859247641895",
                  "-0.0117721878053695",
                  "0.0061272175638169",
                  "-0.0029476641964003",
                  "0.0091849919857954",
                  "-0.000131790557161",
                  "-0.0120267838235386"
                ],
                [
                  "8",
                  "-0.0010120848881359",
                  "0.0024812166932336",
                  "0.0035041249742632",
                  "-0.0069755262807447",
                  "0.0086474388801071",
                  "0.0026806080427706",
                  "0.0004330507043893",
                  "0.0087186807591514",
                  "0.0031299477105209",
                  "0.0053822758704985"
                ],
                [
                  "9",
                  "-0.0058882632826759",
                  "0.0086401851919235",
                  "-0.0221484128379786",
                  "0.0238856241357835",
                  "0.0259652816719893",
                  "0.0082087756460741",
                  "0.0242928137686537",
                  "-0.0070076549828989",
                  "0.0123940184314962",
                  "-0.0019966749014345"
                ]
              ],
              "shape": {
                "columns": 10,
                "rows": 10
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.008801</td>\n",
              "      <td>-0.007235</td>\n",
              "      <td>0.005716</td>\n",
              "      <td>-0.022118</td>\n",
              "      <td>-0.000451</td>\n",
              "      <td>0.019778</td>\n",
              "      <td>0.011677</td>\n",
              "      <td>0.006789</td>\n",
              "      <td>-0.007947</td>\n",
              "      <td>0.015674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.006459</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-0.004770</td>\n",
              "      <td>-0.002964</td>\n",
              "      <td>-0.024045</td>\n",
              "      <td>-0.000303</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>-0.001789</td>\n",
              "      <td>-0.005717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.000141</td>\n",
              "      <td>-0.012624</td>\n",
              "      <td>0.017270</td>\n",
              "      <td>-0.001190</td>\n",
              "      <td>-0.000807</td>\n",
              "      <td>0.013633</td>\n",
              "      <td>0.013879</td>\n",
              "      <td>-0.004942</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.009374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.002734</td>\n",
              "      <td>0.007075</td>\n",
              "      <td>-0.001494</td>\n",
              "      <td>-0.021819</td>\n",
              "      <td>-0.005781</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>-0.000912</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>-0.009696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.016654</td>\n",
              "      <td>0.011078</td>\n",
              "      <td>0.010660</td>\n",
              "      <td>0.006292</td>\n",
              "      <td>-0.009847</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>-0.001830</td>\n",
              "      <td>0.005391</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>-0.010023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.004348</td>\n",
              "      <td>-0.013229</td>\n",
              "      <td>0.011798</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>-0.003068</td>\n",
              "      <td>-0.006986</td>\n",
              "      <td>0.007642</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.006425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.005103</td>\n",
              "      <td>-0.005451</td>\n",
              "      <td>-0.001848</td>\n",
              "      <td>0.018525</td>\n",
              "      <td>0.008415</td>\n",
              "      <td>0.008715</td>\n",
              "      <td>-0.021363</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.005980</td>\n",
              "      <td>-0.005855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.019075</td>\n",
              "      <td>0.010531</td>\n",
              "      <td>-0.010644</td>\n",
              "      <td>0.014686</td>\n",
              "      <td>-0.011772</td>\n",
              "      <td>0.006127</td>\n",
              "      <td>-0.002948</td>\n",
              "      <td>0.009185</td>\n",
              "      <td>-0.000132</td>\n",
              "      <td>-0.012027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.001012</td>\n",
              "      <td>0.002481</td>\n",
              "      <td>0.003504</td>\n",
              "      <td>-0.006976</td>\n",
              "      <td>0.008647</td>\n",
              "      <td>0.002681</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.008719</td>\n",
              "      <td>0.003130</td>\n",
              "      <td>0.005382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.005888</td>\n",
              "      <td>0.008640</td>\n",
              "      <td>-0.022148</td>\n",
              "      <td>0.023886</td>\n",
              "      <td>0.025965</td>\n",
              "      <td>0.008209</td>\n",
              "      <td>0.024293</td>\n",
              "      <td>-0.007008</td>\n",
              "      <td>0.012394</td>\n",
              "      <td>-0.001997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.008801 -0.007235  0.005716 -0.022118 -0.000451  0.019778  0.011677   \n",
              "1  0.011468  0.006459 -0.000006 -0.004770 -0.002964 -0.024045 -0.000303   \n",
              "2 -0.000141 -0.012624  0.017270 -0.001190 -0.000807  0.013633  0.013879   \n",
              "3 -0.002734  0.007075 -0.001494 -0.021819 -0.005781  0.008696 -0.000912   \n",
              "4 -0.016654  0.011078  0.010660  0.006292 -0.009847  0.001006 -0.001830   \n",
              "5 -0.004348 -0.013229  0.011798  0.009824 -0.003068 -0.006986  0.007642   \n",
              "6  0.005103 -0.005451 -0.001848  0.018525  0.008415  0.008715 -0.021363   \n",
              "7  0.019075  0.010531 -0.010644  0.014686 -0.011772  0.006127 -0.002948   \n",
              "8 -0.001012  0.002481  0.003504 -0.006976  0.008647  0.002681  0.000433   \n",
              "9 -0.005888  0.008640 -0.022148  0.023886  0.025965  0.008209  0.024293   \n",
              "\n",
              "          7         8         9  \n",
              "0  0.006789 -0.007947  0.015674  \n",
              "1  0.004872 -0.001789 -0.005717  \n",
              "2 -0.004942  0.010545  0.009374  \n",
              "3  0.000977  0.008981 -0.009696  \n",
              "4  0.005391  0.004360 -0.010023  \n",
              "5  0.000893 -0.007043 -0.006425  \n",
              "6  0.001045  0.005980 -0.005855  \n",
              "7  0.009185 -0.000132 -0.012027  \n",
              "8  0.008719  0.003130  0.005382  \n",
              "9 -0.007008  0.012394 -0.001997  "
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z_train_tfidf_lap.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bcfa931",
      "metadata": {
        "id": "2bcfa931",
        "outputId": "a72c7d75-04fe-433d-992c-9ef22bd31811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "       Date   Company                                              Title  \\\n",
            "0  20170101  [ATC.AE]  [press, release, murphy, owner, kxly, abc, cha...   \n",
            "1  20170101    [ARKR]  [ark, restaurant, apos, ceo, weinstein, result...   \n",
            "2  20170101       [F]  [like, futurist, be, prepare, totally, unexpec...   \n",
            "3  20170101    [NCOM]  [press, release, national, commerce, corporati...   \n",
            "4  20170101     [EDE]  [press, release, district, electric, company, ...   \n",
            "\n",
            "                                             Article  Compound_Return  \\\n",
            "0  [remove, it, programming, wire, despite, willi...              NaN   \n",
            "1                             [from, seek, earnings]         0.070817   \n",
            "2  [by, in, resident, futurist, lead, team, imagi...         0.076860   \n",
            "3  [part, family, globe, ncc, parent, company, he...        -0.020161   \n",
            "4  [wire, closing, today, merger, company, subsid...              NaN   \n",
            "\n",
            "   Day1_Return  \n",
            "0          NaN  \n",
            "1     0.008468  \n",
            "2     0.046068  \n",
            "3    -0.004098  \n",
            "4          NaN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27413 entries, 0 to 27412\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Date             27413 non-null  object \n",
            " 1   Company          27413 non-null  object \n",
            " 2   Title            27413 non-null  object \n",
            " 3   Article          27413 non-null  object \n",
            " 4   Compound_Return  21534 non-null  float64\n",
            " 5   Day1_Return      21604 non-null  float64\n",
            "dtypes: float64(2), object(4)\n",
            "memory usage: 1.3+ MB\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gpapa\\AppData\\Local\\Temp\\ipykernel_27504\\1126090839.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  data = pickle.load(f)\n"
          ]
        }
      ],
      "source": [
        "# Read the pickle file\n",
        "file_path = r\"C:\\Users\\gpapa\\OneDrive\\My Life\\Education\\2025 YALE MMS\\Term_4S\\T4_E_Adv_Lin_Algebra_p2\\final_project\\DJN_2017-01.pkl\"\n",
        "with open(file_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(type(data))\n",
        "if isinstance(data, pd.DataFrame):\n",
        "    print(data.head())\n",
        "    print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f3c1d4",
      "metadata": {
        "id": "92f3c1d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# df = pd.read_csv(\"path/to/your/dataset.csv\")   # <- uncomment & adapt if you haven’t loaded it yet\n",
        "assert {'Article', 'Day1_Return'}.issubset(data.columns), \"Missing one of the required columns.\"\n",
        "\n",
        "# 3. Keep only observations with a target value\n",
        "clean_df = data.dropna(subset=['Day1_Return']).reset_index(drop=True)\n",
        "\n",
        "base_unique = clean_df.drop_duplicates(subset=\"Article\").copy()\n",
        "# create a hashable key\n",
        "base_unique[\"Article_key\"] = base_unique[\"Article\"].apply(tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cead2ea",
      "metadata": {
        "id": "2cead2ea"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 1. SPLIT: preserve indices\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "X = base_unique[\"Article_key\"]       # use the tuple key directly\n",
        "y = base_unique[\"Day1_Return\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, shuffle=True\n",
        ")\n",
        "# note: X_train.index and X_test.index are subsets of base_unique.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca2223f",
      "metadata": {
        "id": "cca2223f",
        "outputId": "f4ed9a17-6b9d-4f7d-e764-d2c719ad36d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2083     (operate, direct, insider, surrender, share, b...\n",
              "10443    (more, follow, press, release, market, drive, ...\n",
              "18941    (investigate, nyse, so, globe, today, announce...\n",
              "3578     (salesforcecom, inc, form, file, reflect, inte...\n",
              "3203     (wire, today, announce, former, member, member...\n",
              "Name: Article_key, dtype: object"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4edc7f5d",
      "metadata": {
        "id": "4edc7f5d"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 2. EMBED & DECOMPOSE (PCA example shown)\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# (you already have Z_train_pca and Z_test_pca as NumPy arrays of shape\n",
        "#  (len(X_train), 10) and (len(X_test), 10), respectively)\n",
        "\n",
        "def make_aligned_dfs(Ztr: np.ndarray,\n",
        "                     Zte: np.ndarray,\n",
        "                     train_idx: pd.Index,\n",
        "                     test_idx: pd.Index,\n",
        "                     prefix: str,\n",
        "                     k: int = 10\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Turn train/test factor arrays into indexed DataFrames with named cols.\n",
        "\n",
        "    Ztr: (n_train × k) factor array\n",
        "    Zte: (n_test  × k) factor array\n",
        "    train_idx: Index for train rows\n",
        "    test_idx : Index for test  rows\n",
        "    prefix   : e.g. \"pca\", \"lap\", \"diff\"\n",
        "    k        : number of components\n",
        "    \"\"\"\n",
        "    cols = [f\"{prefix}_{i+1}\" for i in range(k)]\n",
        "    df_tr = pd.DataFrame(Ztr, index=train_idx, columns=cols)\n",
        "    df_te = pd.DataFrame(Zte, index=test_idx,  columns=cols)\n",
        "    return df_tr, df_te\n",
        "\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "# simple‑avg sets\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "Ztr_avg_pca,  Zte_avg_pca  = make_aligned_dfs(Z_train_simple_pca.values,  Z_test_simple_pca.values,\n",
        "                                          X_train.index, X_test.index, \"pca\")\n",
        "Ztr_avg_lap,  Zte_avg_lap  = make_aligned_dfs(Z_train_simple_lap.values,  Z_test_simple_lap.values,\n",
        "                                          X_train.index, X_test.index, \"lap\")\n",
        "Ztr_avg_diff, Zte_avg_diff = make_aligned_dfs(Z_train_simple_diff.values, Z_test_simple_diff.values,\n",
        "                                          X_train.index, X_test.index, \"diff\")\n",
        "\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "# tfidf sets\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "Ztr_tfidf_pca,  Zte_tfdidf_pca  = make_aligned_dfs(Z_train_tfidf_pca.values,  Z_test_tfidf_pca.values,\n",
        "                                          X_train.index, X_test.index, \"pca\")\n",
        "Ztr_tfidf_lap,  Zte_tfidf_lap  = make_aligned_dfs(Z_train_tfidf_lap.values,  Z_test_tfidf_lap.values,\n",
        "                                          X_train.index, X_test.index, \"lap\")\n",
        "Ztr_tfidf_diff, Zte_tidf_diff = make_aligned_dfs(Z_train_tfidf_diff.values, Z_test_tfidf_diff.values,\n",
        "                                          X_train.index, X_test.index, \"diff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd92b77",
      "metadata": {
        "id": "edd92b77"
      },
      "outputs": [],
      "source": [
        "# only keep **X** PCs for each method\n",
        "\n",
        "ArrayLike = Union[pd.DataFrame, np.ndarray]\n",
        "\n",
        "def slice_components(\n",
        "    Ztr: ArrayLike,\n",
        "    Zte: ArrayLike,\n",
        "    n_components: int\n",
        ") -> Tuple[ArrayLike, ArrayLike]:\n",
        "    \"\"\"\n",
        "    Keep only the first `n_components` columns of Ztr and Zte.\n",
        "    Works for either DataFrames or ndarrays.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (Ztr_sliced, Zte_sliced)\n",
        "    \"\"\"\n",
        "    if isinstance(Ztr, pd.DataFrame) and isinstance(Zte, pd.DataFrame):\n",
        "        return Ztr.iloc[:, :n_components], Zte.iloc[:, :n_components]\n",
        "    else:\n",
        "        # assume NumPy arrays\n",
        "        return Ztr[:, :n_components], Zte[:, :n_components]\n",
        "\n",
        "# USAGE -- KEEP 3 PCs\n",
        "Ztr_avg_pca,  Zte_avg_pca  = slice_components(Ztr_avg_pca,  Zte_avg_pca,  3)\n",
        "Ztr_avg_lap,  Zte_avg_lap  = slice_components(Ztr_avg_lap,  Zte_avg_lap,  3)\n",
        "Ztr_avg_diff, Zte_avg_diff = slice_components(Ztr_avg_diff, Zte_avg_diff, 3)\n",
        "\n",
        "Ztr_tfidf_pca,  Zte_tfdidf_pca  = slice_components(Ztr_tfidf_pca,  Zte_tfdidf_pca,  3)\n",
        "Ztr_tfidf_lap,  Zte_tfidf_lap  = slice_components(Ztr_tfidf_lap,  Zte_tfidf_lap,  3)\n",
        "Ztr_tfidf_diff, Zte_tidf_diff = slice_components(Ztr_tfidf_diff, Zte_tidf_diff, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bade3ef9",
      "metadata": {
        "id": "bade3ef9"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the full base_unique embedding matrix by concatenating train + test\n",
        "Z_avg_base_pca  = pd.concat([Ztr_avg_pca,  Zte_avg_pca], axis=0)\n",
        "Z_avg_base_lap  = pd.concat([Ztr_avg_lap,  Zte_avg_lap], axis=0)\n",
        "Z_avg_base_diff = pd.concat([Ztr_avg_diff, Zte_avg_diff], axis=0)\n",
        "# Now Z_simple_base_pca.index == base_unique.index\n",
        "# and Z_simple_base_pca[pca_cols] holds the 10-d vectors for each unique-article\n",
        "\n",
        "Z_tfidf_base_pca  = pd.concat([Ztr_tfidf_pca,  Zte_tfdidf_pca], axis=0)\n",
        "Z_tfidf_base_lap  = pd.concat([Ztr_tfidf_lap,  Zte_tfidf_lap], axis=0)\n",
        "Z_tfidf_base_diff = pd.concat([Ztr_tfidf_diff, Zte_tidf_diff], axis=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646fc7c5",
      "metadata": {
        "id": "646fc7c5",
        "outputId": "169d5f47-9c54-4310-c644-6f599951981d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "pca_1",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pca_2",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pca_3",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "d1869512-175c-4487-8abe-ea33c8677c52",
              "rows": [
                [
                  "2083",
                  "-0.6436345392015044",
                  "-0.2647561557793441",
                  "0.2030187869409045"
                ],
                [
                  "10443",
                  "-0.3444263988341272",
                  "0.243451712089909",
                  "0.0945822852495988"
                ],
                [
                  "18941",
                  "-0.0526085329680805",
                  "-0.0863375909089056",
                  "0.6707553255036826"
                ],
                [
                  "3578",
                  "-0.2022437711026046",
                  "-0.0102701918484122",
                  "0.1840022833741456"
                ],
                [
                  "3203",
                  "0.6054845931237063",
                  "0.1604938374785373",
                  "0.2472206853921194"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pca_1</th>\n",
              "      <th>pca_2</th>\n",
              "      <th>pca_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2083</th>\n",
              "      <td>-0.643635</td>\n",
              "      <td>-0.264756</td>\n",
              "      <td>0.203019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10443</th>\n",
              "      <td>-0.344426</td>\n",
              "      <td>0.243452</td>\n",
              "      <td>0.094582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18941</th>\n",
              "      <td>-0.052609</td>\n",
              "      <td>-0.086338</td>\n",
              "      <td>0.670755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3578</th>\n",
              "      <td>-0.202244</td>\n",
              "      <td>-0.010270</td>\n",
              "      <td>0.184002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>0.605485</td>\n",
              "      <td>0.160494</td>\n",
              "      <td>0.247221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          pca_1     pca_2     pca_3\n",
              "2083  -0.643635 -0.264756  0.203019\n",
              "10443 -0.344426  0.243452  0.094582\n",
              "18941 -0.052609 -0.086338  0.670755\n",
              "3578  -0.202244 -0.010270  0.184002\n",
              "3203   0.605485  0.160494  0.247221"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z_avg_base_pca.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98ee915",
      "metadata": {
        "id": "f98ee915"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 4. MERGE back to clean_df\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def broadcast_embeddings(\n",
        "    clean_df: pd.DataFrame,\n",
        "    base_unique: pd.DataFrame,\n",
        "    Z_base: pd.DataFrame,\n",
        "    prefix: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Attach the embedding columns in Z_base (indexed by base_unique.index)\n",
        "    back onto every row of clean_df that shares the same Article_key.\n",
        "\n",
        "    clean_df     : full DataFrame with duplicates\n",
        "    base_unique  : deduped DataFrame with Article_key column\n",
        "    Z_base       : DataFrame of embeddings, indexed the same as base_unique\n",
        "    prefix       : e.g. 'avg_pca', 'avg_lap', 'avg_diff',\n",
        "                   'tfidf_pca', etc.  Used only in warnings.\n",
        "    \"\"\"\n",
        "    # 1️⃣  ensure the key exists\n",
        "    if \"Article_key\" not in clean_df.columns:\n",
        "        clean_df = clean_df.copy()\n",
        "        clean_df[\"Article_key\"] = clean_df[\"Article\"].apply(tuple)\n",
        "\n",
        "    # 2️⃣  build a lookup: Article_key → embeddings\n",
        "    lookup = base_unique[[\"Article_key\"]].join(Z_base)\n",
        "\n",
        "    # 3️⃣  merge onto clean_df\n",
        "    merged = clean_df.merge(\n",
        "        lookup,\n",
        "        on=\"Article_key\",\n",
        "        how=\"left\",\n",
        "        validate=\"many_to_one\"  # each clean_df row matches exactly one lookup row\n",
        "    )\n",
        "\n",
        "    # 4️⃣  drop the helper key\n",
        "    return merged.drop(columns=\"Article_key\")\n",
        "\n",
        "\n",
        "# —————————————————————————————————————————————————————————————————————\n",
        "# Now call it for your six embedding bases\n",
        "# —————————————————————————————————————————————————————————————————————\n",
        "\n",
        "# Simple‑avg\n",
        "clean_with_Z_avg_pca  = broadcast_embeddings(clean_df, base_unique, Z_avg_base_pca,  \"avg_pca\")\n",
        "clean_with_Z_avg_lap  = broadcast_embeddings(clean_df, base_unique, Z_avg_base_lap,  \"avg_lap\")\n",
        "clean_with_Z_avg_diff = broadcast_embeddings(clean_df, base_unique, Z_avg_base_diff, \"avg_diff\")\n",
        "\n",
        "# TF‑IDF\n",
        "clean_with_Z_tf_pca  = broadcast_embeddings(clean_df, base_unique, Z_tfidf_base_pca,  \"tfidf_pca\")\n",
        "clean_with_Z_tf_lap  = broadcast_embeddings(clean_df, base_unique, Z_tfidf_base_lap,  \"tfidf_lap\")\n",
        "clean_with_Z_tf_diff = broadcast_embeddings(clean_df, base_unique, Z_tfidf_base_diff, \"tfidf_diff\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f9d5a4",
      "metadata": {
        "id": "10f9d5a4",
        "outputId": "4a836da6-c883-416f-c2c3-38e9aee429da"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Date",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Company",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "Title",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "Compound_Return",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "Day1_Return",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "lap_1",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "lap_2",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "lap_3",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "4c307a33-ca2b-4366-842c-3e273ae6a80f",
              "rows": [
                [
                  "0",
                  "20170101",
                  "['ARKR']",
                  "['ark', 'restaurant', 'apos', 'ceo', 'weinstein', 'result', 'earnings', 'call', 'transcript', 'arkr']",
                  "['from', 'seek', 'earnings']",
                  "0.07081665749614441",
                  "0.008468",
                  "7.28942361297177e-05",
                  "-2.5065154938320032e-05",
                  "8.912409736221614e-06"
                ],
                [
                  "1",
                  "20170101",
                  "['F']",
                  "['like', 'futurist', 'be', 'prepare', 'totally', 'unexpected']",
                  "['by', 'in', 'resident', 'futurist', 'lead', 'team', 'imagine', 'happen', 'economic', 'shock', 'increase', 'price', 'gasoline', 'lead', 'crash', 'automotive', 'sale', 'with', 'economic', 'crash', 'subsequent', 'bailout', 'auto', 'industry', 'seem', 'scenario', 'come', 'true', 'but', 'team', 'really', 'predict', 'future', 'always', 'feel', 'compelled', 'people', 'group', 'also', 'spend', 'time', 'albeit', 'one', 'talk', 'happen', 'alien', 'religion', 'react', 'way', 'lead', 'society', 'crumble', 'moral', 'infrastructure', 'say', 'predict', 'future', 'apost', 'futurist', 'and', 'funny', 'way', 'aposs', 'make', 'work', 'vital', 'many', 'futurist', 'convince', 'ever', 'everyone', 'need', 'start', 'think', 'way', 'what', 'futurists', 'actually', 'facilitate', 'group', 'people', 'work', 'highly', 'structure', 'sometimes', 'month', 'process', 'come', 'many', 'hypothetical', 'future', 'order', 'prepare', 'less', 'anything', 'until', 'futurism', 'practice', 'mostly', 'large', 'organization', 'make', 'decision', 'might', 'become', 'policy', 'product', 'many', 'year', 'come', 'big', 'company', 'like', 'well', 'government', 'agency', 'especially', 'employ', 'futurist', 'academic', 'discipline', 'taught', 'dozen', 'university', 'around', 'world', 'in', 'current', 'moment', 'political', 'economic', 'uncertainty', 'combine', 'technological', 'change', 'aposs', 'clear', 'aposre', 'go', 'make', 'passenger', 'say', 'futurist', 'year', 'creator', 'how', 'to', 'i', 'recently', 'spend', 'day', 'futuring', 'whose', 'recent', 'book', 'signal', 'be', 'good', 'introduction', 'topic', 'we', 'decide', 'examine', 'future', 'self', 'drive', 'vehicle', 'first', 'thing', 'futurism', 'surprise', 'practitioner', 'apost', 'much', 'technological', 'change', 'at', 'least', 'first', 'they', 'start', 'factor', 'drive', 'change', 'wealth', 'distribution', 'education', 'demography', 'politics', 'environment', 'medium', 'this', 'make', 'sense', 'one', 'predict', 'rise', 'focus', 'simply', 'capability', 'smartphones', 'internet', 'another', 'surprise', 'futurists', 'relentlessly', 'critical', 'assumption', 'aposre', 'do', 'come', 'wild', 'eye', 'notion', 'change', 'might', 'arise', 'result', 'various', 'force', 'tear', 'apart', 'work', 'when', 'i', 'settle', 'future', 'self', 'drive', 'bus', 'transit', 'first', 'imagine', 'group', 'service', 'propose', 'today', 'might', 'death', 'bus', 'stop', 'pre', 'plan', 'bus', 'route', 'but', 'pick', 'apart', 'assumption', 'become', 'clear', 'predictable', 'nature', 'commute', 'bus', 'route', 'route', 'simply', 'well', 'informed', 'often', 'people', 'actually', 'need', 'transit', 'futurist', 'even', 'though', 'work', 'much', 'art', 'science', 'attempt', 'make', 'rigorously', 'quantifiable', 'we', 'aggressive', 'computation', 'try', 'ask', 'difference', 'thing', 'long', 'term', 'make', 'choice', 'today', 'say', 'principal', 'researcher', 'policy', 'tank', 'that', 'anything', 'model', 'economic', 'environmental', 'factor', 'affect', 'construction', 'new', 'reservoir', 'aqueduct', 'whether', 'agree', 'insure', 'company', 'sell', 'terrorism', 'insurance', 'one', 'thing', 'futurists', 'i', 'talk', 'common', 'disdain', 'anyone', 'willing', 'attempt', 'predict', 'future', 'in', 'futuring', 'circle', 'paradoxically', 'amateur', 'actually', 'practice', 'futurism', 'even', 'day', 'show', 'reason', 'future', 'confound', 'fact', 'anything', 'happen', 'unexpected', 'event', 'rapidly', 'compound', 'one', 'another', 'this', 'lead', 'second', 'nth', 'order', 'effect', 'seem', 'completely', 'beyond', 'realm', 'plausibility', 'happen', 'impossibility', 'predict', 'financial', 'crisis', 'war', 'technological', 'revolution', 'but', 'least', 'futurist', 'lend', 'u', 'sort', 'mental', 'flexibility', 'well', 'ability', 'trend', 'otherwise', 'easily', 'dismiss', 'example', 'time', 'convince', 'rise', 'drone', 'might', 'day', 'lead', 'height', 'regulation', 'building', 'gray', 'fray', 'support', 'network', 'lead', 'social', 'acceptance', 'euthanasia', 'and', 'also', 'course', 'affect', 'car', 'sale', 'longer', 'futurist', 'say', 'like', 'ability', 'make', 'budget', 'critically', 'aposs', 'skill', 'anyone', 'make', 'range', 'decision', 'acquire', 'do', 'scale', 'require', 'corporation', 'might', 'require', 'week', 'effort', 'team', 'people', 'individual', 'much', 'simpler', 'it', 'apost', 'require', 'complicated', 'nine', 'month', 'process', 'say', 'add', 'we', 'minute', 'table', 'come', 'kind', 'interesting', 'realization', 'christophermims']",
                  "0.07685995683738067",
                  "0.046068",
                  "-0.0055633458491597",
                  "-0.0151601375425992",
                  "0.0014546380870862"
                ],
                [
                  "2",
                  "20170101",
                  "['NCOM']",
                  "['press', 'release', 'national', 'commerce', 'corporation', 'announces', 'close', 'private', 'bancshares', 'inc', 'merger']",
                  "['part', 'family', 'globe', 'ncc', 'parent', 'company', 'headquarter', 'announce', 'today', 'completion', 'merger', 'pbi', 'parent', 'company', 'merger', 'merge', 'result', 'merger', 'part', 'continue', 'operate', 'trade', 'name', 'management', 'team', 'continue', 'extremely', 'excited', 'partner', 'team', 'include', 'say', 'team', 'strong', 'track', 'record', 'customer', 'growth', 'credit', 'believe', 'excellent', 'cultural', 'fit', 'company', 'we', 'client', 'shareowner', 'employee', 'we', 'look', 'work', 'together', 'continue', 'build', 'company', 'spoke', 'merger', 'say', 'we', 'pleased', 'friend', 'combine', 'bank', 'enables', 'u', 'continue', 'serve', 'client', 'part', 'large', 'organization', 'one', 'root', 'community', 'banking', 'we', 'team', 'many', 'year', 'believe', 'merger', 'beneficial', 'client', 'stockholder', 'employee', 'community', 'serve', 'we', 'also', 'excite', 'partner', 'team', 'market', 'this', 'great', 'new', 'chapter', 'come', 'heel', 'recent', 'tenth', 'anniversary', 'in', 'conjunction', 'closing', 'merger', 'director', 'appoint', 'recognize', 'community', 'leader', 'law', 'firm', 'act', 'financial', 'adviser', 'act', 'legal', 'adviser', 'act', 'financial', 'adviser', 'act', 'legal', 'adviser', 'about', 'nasdaq', 'ncom', 'corporation', 'financial', 'hold', 'company', 'headquarter', 'operation', 'conduct', 'company', 'aposs', 'wholly', 'own', 'subsidiary', 'currently', 'operate', 'seven', 'full', 'banking', 'office', 'full', 'banking', 'office', 'central', 'northeast', 'include', 'trade', 'name', 'two', 'full', 'banking', 'office', 'include', 'trade', 'name', 'provide', 'broad', 'array', 'financial', 'service', 'commercial', 'consumer', 'customer', 'own', 'majority', 'stake', 'transaction', 'base', 'company', 'base', 'provide', 'factor', 'invoice', 'collection', 'account', 'receivable', 'management', 'service', 'transportation', 'company', 'automotive', 'part', 'provider', 'throughout', 'part', 'file', 'periodic', 'report', 'filing', 'may', 'obtain', 'website', 'more', 'information', 'may', 'obtain', 'forward', 'look', 'statement', 'certain', 'statement', 'contain', 'press', 'release', 'statement', 'historical', 'fact', 'constitute', 'look', 'statement', 'claim', 'protection', 'safe', 'harbor', 'provision', 'contain', 'notwithstanding', 'statement', 'specifically', 'identify', 'in', 'addition', 'certain', 'statement', 'may', 'contain', 'future', 'filing', 'press', 'release', 'write', 'statement', 'make', 'approval', 'statement', 'historical', 'fact', 'constitute', 'look', 'statement', 'within', 'mean', 'believe', 'similar', 'expression', 'intend', 'identify', 'look', 'statement', 'exclusive', 'mean', 'identify', 'statement', 'forward', 'look', 'statement', 'include', 'representation', 'involve', 'risk', 'uncertainty', 'may', 'cause', 'actual', 'result', 'differ', 'materially', 'future', 'result', 'performance', 'achievement', 'express', 'implied', 'statement', 'these', 'risk', 'uncertainty', 'difficulty', 'delay', 'unanticipated', 'cost', 'integrate', 'business', 'realize', 'expect', 'cost', 'saving', 'benefit', 'business', 'disruption', 'result', 'integration', 'merging', 'organization', 'include', 'possible', 'loss', 'customer', 'diversion', 'management', 'time', 'address', 'transaction', 'relate', 'issue', 'change', 'asset', 'credit', 'risk', 'result', 'merger', 'these', 'risk', 'also', 'number', 'factor', 'related', 'business', 'banking', 'business', 'generally', 'include', 'various', 'risk', 'stockholder', 'receive', 'dividend', 'risk', 'ability', 'pursue', 'growth', 'opportunities', 'various', 'risk', 'price', 'volatility', 'common', 'stock', 'ability', 'incur', 'additional', 'financial', 'obligation', 'future', 'risk', 'associate', 'possible', 'pursuit', 'future', 'acquisition', 'economic', 'condition', 'current', 'area', 'include', 'new', 'area', 'create', 'merger', 'system', 'failures', 'loss', 'large', 'customer', 'disruption', 'relationships', 'party', 'vendor', 'loss', 'management', 'personnel', 'inability', 'attract', 'retain', 'highly', 'qualified', 'management', 'personnel', 'future', 'change', 'governmental', 'legislation', 'regulation', 'govern', 'banking', 'cost', 'regulatory', 'compliance', 'impact', 'legislation', 'regulatory', 'change', 'bank', 'industry', 'liability', 'compliance', 'cost', 'regard', 'banking', 'regulation', 'forward', 'look', 'statement', 'make', 'press', 'release', 'elsewhere', 'speak', 'date', 'statement', 'make', 'you', 'advised', 'read', 'risk', 'factor', 'recently', 'file', 'annual', 'subsequent', 'filing', 'available', 'website', 'maintain', 'access', 'information', 'available', 'risk', 'uncertainty', 'arise', 'time', 'time', 'impossible', 'predict', 'event', 'may', 'affect', 'anticipated', 'result', 'duty', 'intend', 'update', 'revise', 'look', 'statement', 'press', 'release', 'except', 'may', 'require', 'law', 'in', 'light', 'risk', 'uncertainties', 'reader', 'keep', 'mind', 'look', 'statement', 'make', 'press', 'release', 'may', 'occur', 'all', 'information', 'present', 'herein', 'date', 'release', 'unless', 'otherwise', 'note', 'rmurray', 'division']",
                  "-0.020161005019676215",
                  "-0.004098",
                  "0.0013501458030393",
                  "-0.0125017679514179",
                  "0.0174117976247336"
                ],
                [
                  "3",
                  "20170101",
                  "['SCOR']",
                  "['press', 'release', 'comscore', 'report', 'north', 'movie', 'box', 'office', 'hit', 'new', 'all', 'time', 'record']",
                  "['comscore', 'report', 'north', 'hit', 'two', 'last', 'prnewswire', 'comscore', 'nasdaq', 'global', 'box', 'office', 'reporting', 'today', 'announce', 'north', 'box', 'office', 'set', 'new', 'record', 'billion', 'make', 'high', 'earn', 'year', 'movie', 'history', 'wide', 'assortment', 'movie', 'blockbuster', 'side', 'ledger', 'auspicious', 'crop', 'small', 'scale', 'film', 'bring', 'enthusiastic', 'patron', 'movie', 'theater', 'across', 'throughout', 'year', 'give', 'industry', 'big', 'overall', 'revenue', 'north', 'box', 'office', 'history', 'forgetful', 'fish', 'super', 'hero', 'household', 'pet', 'space', 'traveler', 'lead', 'charge', 'year', 'mark', 'incredibly', 'diverse', 'selection', 'film', 'every', 'genre', 'every', 'size', 'scope', 'studio', 'comscore', 'aposs', 'say', 'sparked', 'extraordinary', 'level', 'enthusiasm', 'patron', 'flock', 'incredibly', 'well', 'appoint', 'movie', 'theater', 'list', 'high', 'gross', 'film', 'north', 'box', 'office', 'comscore', 'top', 'movie', 'thru', 'include', 'holdover', 'of', 'of', 'about', 'comscore', 'comscore', 'movie', 'industry', 'aposs', 'census', 'base', 'currency', 'collect', 'process', 'report', 'many', 'people', 'go', 'movie', 'much', 'spend', 'virtually', 'every', 'theater', 'major', 'studio', 'mini', 'major', 'prominent', 'independent', 'film', 'industry', 'use', 'comscore', 'aposs', 'real', 'time', 'geographic', 'specific', 'box', 'office', 'information', 'provide', 'user', 'instant', 'analysis', 'nearly', 'entire', 'domestic', 'box', 'office', 'landscape', 'collect', 'electronic', 'connection', 'thousand', 'theater', 'box', 'office', 'comscore', 'aposs', 'box', 'office', 'intelligence', 'accessible', 'via', 'online', 'report', 'anywhere', 'around', 'world', 'allow', 'user', 'literally', 'watch', 'virtual', 'minute', 'minute', 'sale', 'ticket', 'about', 'comscore', 'comscore', 'lead', 'cross', 'platform', 'measurement', 'company', 'precisely', 'measure', 'audience', 'brand', 'consumer', 'behavior', 'everywhere', 'comscore', 'complete', 'merger', 'create', 'new', 'cross', 'platform', 'world', 'precision', 'innovation', 'unmatched', 'footprint', 'combine', 'proprietary', 'tv', 'movie', 'intelligence', 'demographic', 'detail', 'quantify', 'consumer', 'apos', 'multiscreen', 'behavior', 'scale', 'this', 'approach', 'help', 'medium', 'company', 'monetize', 'complete', 'audience', 'allow', 'marketer', 'reach', 'audience', 'effectively', 'with', 'client', 'global', 'footprint', 'country', 'comscore', 'delivering', 'future', 'measurement', 'information', 'comscore', 'please', 'comscorecom', 'to', 'version', 'comscore', 'comscore', 'press', 'site']",
                  "0.023540904821481412",
                  "0.025794",
                  "-6.326354210002982e-05",
                  "1.4995635049097807e-05",
                  "-1.9532859664976624e-05"
                ],
                [
                  "4",
                  "20170102",
                  "['TWTR']",
                  "['late', 'twitter', 'exec', 'depart', 'great', 'china', 'head', 'market', 'talk']",
                  "['say', 'depart', 'position', 'manage', 'director', 'late', 'string', 'senior', 'figure', 'leave', 'company', 'base', 'take', 'move', 'company', 'join', 'advertising', 'sale', 'support', 'service', 'move', 'twitter', 'aposs', 'asia', 'pacific', 'headquarters', 'say', 'cut', 'workforce', 'in', 'recent', 'month', 'company', 'aposs', 'operate', 'officer', 'technology', 'officer', 'leave', 'firm', 'newleypurnell']",
                  "0.028675777554302062",
                  "0.025547",
                  "-0.0017956362478195",
                  "-0.0165297230240328",
                  "-0.000957991018122"
                ]
              ],
              "shape": {
                "columns": 9,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Company</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article</th>\n",
              "      <th>Compound_Return</th>\n",
              "      <th>Day1_Return</th>\n",
              "      <th>lap_1</th>\n",
              "      <th>lap_2</th>\n",
              "      <th>lap_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20170101</td>\n",
              "      <td>[ARKR]</td>\n",
              "      <td>[ark, restaurant, apos, ceo, weinstein, result...</td>\n",
              "      <td>[from, seek, earnings]</td>\n",
              "      <td>0.070817</td>\n",
              "      <td>0.008468</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20170101</td>\n",
              "      <td>[F]</td>\n",
              "      <td>[like, futurist, be, prepare, totally, unexpec...</td>\n",
              "      <td>[by, in, resident, futurist, lead, team, imagi...</td>\n",
              "      <td>0.076860</td>\n",
              "      <td>0.046068</td>\n",
              "      <td>-0.005563</td>\n",
              "      <td>-0.015160</td>\n",
              "      <td>0.001455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20170101</td>\n",
              "      <td>[NCOM]</td>\n",
              "      <td>[press, release, national, commerce, corporati...</td>\n",
              "      <td>[part, family, globe, ncc, parent, company, he...</td>\n",
              "      <td>-0.020161</td>\n",
              "      <td>-0.004098</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>-0.012502</td>\n",
              "      <td>0.017412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20170101</td>\n",
              "      <td>[SCOR]</td>\n",
              "      <td>[press, release, comscore, report, north, movi...</td>\n",
              "      <td>[comscore, report, north, hit, two, last, prne...</td>\n",
              "      <td>0.023541</td>\n",
              "      <td>0.025794</td>\n",
              "      <td>-0.000063</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20170102</td>\n",
              "      <td>[TWTR]</td>\n",
              "      <td>[late, twitter, exec, depart, great, china, he...</td>\n",
              "      <td>[say, depart, position, manage, director, late...</td>\n",
              "      <td>0.028676</td>\n",
              "      <td>0.025547</td>\n",
              "      <td>-0.001796</td>\n",
              "      <td>-0.016530</td>\n",
              "      <td>-0.000958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date Company                                              Title  \\\n",
              "0  20170101  [ARKR]  [ark, restaurant, apos, ceo, weinstein, result...   \n",
              "1  20170101     [F]  [like, futurist, be, prepare, totally, unexpec...   \n",
              "2  20170101  [NCOM]  [press, release, national, commerce, corporati...   \n",
              "3  20170101  [SCOR]  [press, release, comscore, report, north, movi...   \n",
              "4  20170102  [TWTR]  [late, twitter, exec, depart, great, china, he...   \n",
              "\n",
              "                                             Article  Compound_Return  \\\n",
              "0                             [from, seek, earnings]         0.070817   \n",
              "1  [by, in, resident, futurist, lead, team, imagi...         0.076860   \n",
              "2  [part, family, globe, ncc, parent, company, he...        -0.020161   \n",
              "3  [comscore, report, north, hit, two, last, prne...         0.023541   \n",
              "4  [say, depart, position, manage, director, late...         0.028676   \n",
              "\n",
              "   Day1_Return     lap_1     lap_2     lap_3  \n",
              "0     0.008468  0.000073 -0.000025  0.000009  \n",
              "1     0.046068 -0.005563 -0.015160  0.001455  \n",
              "2    -0.004098  0.001350 -0.012502  0.017412  \n",
              "3     0.025794 -0.000063  0.000015 -0.000020  \n",
              "4     0.025547 -0.001796 -0.016530 -0.000958  "
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_with_Z_tf_lap.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0122d9",
      "metadata": {
        "id": "ef0122d9",
        "outputId": "1383fa0b-feeb-4e22-d31d-babaa4829f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Alignment check for avg_pca ===\n",
            "✔️  Perfect alignment: no index mismatches.\n",
            "\n",
            "=== Alignment check for avg_lap ===\n",
            "✔️  Perfect alignment: no index mismatches.\n",
            "\n",
            "=== Alignment check for avg_diff ===\n",
            "✔️  Perfect alignment: no index mismatches.\n",
            "\n",
            "=== Alignment check for tfidf_pca ===\n",
            "✔️  Perfect alignment: no index mismatches.\n",
            "\n",
            "=== Alignment check for tfidf_lap ===\n",
            "✔️  Perfect alignment: no index mismatches.\n",
            "\n",
            "=== Alignment check for tfidf_diff ===\n",
            "✔️  Perfect alignment: no index mismatches.\n"
          ]
        }
      ],
      "source": [
        "# Sanity check: make sure the indexes are aligned\n",
        "\n",
        "def check_index_alignment(base_unique: pd.DataFrame,\n",
        "                          embedding_bases: dict[str, pd.DataFrame]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    For each named embedding base, print any index values that appear\n",
        "    in base_unique but not in the embedding, and vice versa.\n",
        "\n",
        "    embedding_bases: mapping from a string name (e.g. \"avg_pca\") to the\n",
        "                     corresponding Z_base DataFrame (indexed by base_unique).\n",
        "    \"\"\"\n",
        "    for name, Z_base in embedding_bases.items():\n",
        "        missing_in_Z = base_unique.index.difference(Z_base.index)\n",
        "        extra_in_Z   = Z_base.index.difference(base_unique.index)\n",
        "        print(f\"\\n=== Alignment check for {name} ===\")\n",
        "        if missing_in_Z.empty and extra_in_Z.empty:\n",
        "            print(\"✔️  Perfect alignment: no index mismatches.\")\n",
        "        else:\n",
        "            if not missing_in_Z.empty:\n",
        "                print(f\"❌ Indices in base_unique but not in {name}: {missing_in_Z.tolist()}\")\n",
        "            if not extra_in_Z.empty:\n",
        "                print(f\"❌ Indices in {name} but not in base_unique: {extra_in_Z.tolist()}\")\n",
        "\n",
        "# —————————————————————————————————————————————————————————————————————\n",
        "# Call it once with all three bases\n",
        "# —————————————————————————————————————————————————————————————————————\n",
        "check_index_alignment(\n",
        "    base_unique,\n",
        "    {\n",
        "        \"avg_pca\":  Z_avg_base_pca,\n",
        "        \"avg_lap\":  Z_avg_base_lap,\n",
        "        \"avg_diff\": Z_avg_base_diff,\n",
        "        \"tfidf_pca\":  Z_tfidf_base_pca,\n",
        "        \"tfidf_lap\":  Z_tfidf_base_lap,\n",
        "        \"tfidf_diff\": Z_tfidf_base_diff,\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6389f64",
      "metadata": {
        "id": "a6389f64"
      },
      "source": [
        "### STEP 2 - RUN REGRESSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405759a8",
      "metadata": {
        "id": "405759a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.metrics       import r2_score, mean_squared_error\n",
        "\n",
        "\n",
        "def get_train_test_Z(clean_with_Z: pd.DataFrame,\n",
        "                     prefix: str,\n",
        "                     X_train_idx: pd.Index,\n",
        "                     X_test_idx: pd.Index\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extract train/test factor matrices from a broadcasted DataFrame.\n",
        "\n",
        "    clean_with_Z   : DataFrame with columns prefix_1…prefix_k\n",
        "    prefix         : e.g. 'avg_pca', 'avg_lap', etc.\n",
        "    X_train_idx    : Index for training rows\n",
        "    X_test_idx     : Index for test rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Ztr, Zte : NumPy arrays of shape (len(X_train), k), (len(X_test), k)\n",
        "    \"\"\"\n",
        "    # find all columns that start with the prefix\n",
        "    cols = [c for c in clean_with_Z.columns if c.startswith(prefix + \"_\")]\n",
        "    Ztr = clean_with_Z.loc[X_train_idx, cols].values\n",
        "    Zte = clean_with_Z.loc[X_test_idx,  cols].values\n",
        "    return Ztr, Zte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff290aa",
      "metadata": {
        "id": "fff290aa"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────\n",
        "# Simple‐avg embeddings (already broadcasted)\n",
        "# ──────────────────────────────────────────────────\n",
        "Ztr_s_pca,  Zte_s_pca  = get_train_test_Z(clean_with_Z_avg_pca,  \"pca\", X_train.index, X_test.index)\n",
        "Ztr_s_lap,  Zte_s_lap  = get_train_test_Z(clean_with_Z_avg_lap,  \"lap\", X_train.index, X_test.index)\n",
        "Ztr_s_diff, Zte_s_diff = get_train_test_Z(clean_with_Z_avg_diff, \"diff\", X_train.index, X_test.index)\n",
        "\n",
        "# ──────────────────────────────────────────────────\n",
        "# TF‑IDF embeddings (already broadcasted)\n",
        "# ──────────────────────────────────────────────────\n",
        "Ztr_t_pca,  Zte_t_pca  = get_train_test_Z(clean_with_Z_tf_pca,  \"pca\", X_train.index, X_test.index)\n",
        "Ztr_t_lap,  Zte_t_lap  = get_train_test_Z(clean_with_Z_tf_lap,  \"lap\", X_train.index, X_test.index)\n",
        "Ztr_t_diff, Zte_t_diff = get_train_test_Z(clean_with_Z_tf_diff, \"diff\", X_train.index, X_test.index)\n",
        "\n",
        "# Now you have six (Ztr, Zte) pairs ready for regression:\n",
        "methods = [\n",
        "    (\"Simple‑avg PCA\",       Ztr_s_pca,  Zte_s_pca),\n",
        "    (\"Simple‑avg Laplacian\", Ztr_s_lap,  Zte_s_lap),\n",
        "    (\"Simple‑avg Diffusion\", Ztr_s_diff, Zte_s_diff),\n",
        "    (\"TF‑IDF PCA\",           Ztr_t_pca,  Zte_t_pca),\n",
        "    (\"TF‑IDF Laplacian\",     Ztr_t_lap,  Zte_t_lap),\n",
        "    (\"TF‑IDF Diffusion\",     Ztr_t_diff, Zte_t_diff),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d36d39e",
      "metadata": {
        "id": "7d36d39e",
        "outputId": "57f39b6c-0a04-47dd-b5d4-ff1eaa4fd00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple‑avg PCA shapes: (8378, 3) (2793, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"Simple‑avg PCA shapes:\", Ztr_s_pca.shape, Zte_s_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ed6e14",
      "metadata": {
        "id": "73ed6e14",
        "outputId": "dbce33a2-37dc-4ddf-d7c1-47ced861ec56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simple‑avg PCA:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n",
            "\n",
            "Simple‑avg Laplacian:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n",
            "\n",
            "Simple‑avg Diffusion:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n",
            "\n",
            "TF‑IDF PCA:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n",
            "\n",
            "TF‑IDF Laplacian:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n",
            "\n",
            "TF‑IDF Diffusion:\n",
            "  Training NaNs: 0\n",
            "  Testing  NaNs: 0\n"
          ]
        }
      ],
      "source": [
        "# SANITY CHECK\n",
        "\n",
        "def check_nans_in_embeddings(methods):\n",
        "    \"\"\"\n",
        "    Given a list of (name, Ztr, Zte) tuples, report NaN counts in each.\n",
        "    Returns a dict mapping method names to {'Train': count, 'Test': count}.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for name, Ztr, Zte in methods:\n",
        "        train_nans = np.isnan(Ztr).sum()\n",
        "        test_nans  = np.isnan(Zte).sum()\n",
        "        results[name] = {'Train': train_nans, 'Test': test_nans}\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Training NaNs: {train_nans}\")\n",
        "        print(f\"  Testing  NaNs: {test_nans}\")\n",
        "    return results\n",
        "\n",
        "# Call the function\n",
        "nan_results = check_nans_in_embeddings(methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d30d99",
      "metadata": {
        "id": "a0d30d99",
        "outputId": "f6cae960-7917-4555-fc45-e1d92d1816d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        OOS R²       MSE\n",
            "Method                                  \n",
            "Simple‑avg PCA       -0.000509  0.000563\n",
            "Simple‑avg Laplacian -0.000260  0.000563\n",
            "Simple‑avg Diffusion  0.000752  0.000563\n",
            "TF‑IDF PCA           -0.001027  0.000564\n",
            "TF‑IDF Laplacian     -0.000267  0.000563\n",
            "TF‑IDF Diffusion     -0.000593  0.000563\n"
          ]
        }
      ],
      "source": [
        "# 3️⃣  Run OLS/Ridge regressions & report OOS R² and MSE\n",
        "# -------------------------------------------------\n",
        "results = []\n",
        "for name, Ztr, Zte in methods:\n",
        "    mdl  = LinearRegression().fit(Ztr, y_train)   # SWAP for Ridge(alpha=1) OR LinearRegression()\n",
        "    pred = mdl.predict(Zte)\n",
        "    results.append({\n",
        "        \"Method\": name,\n",
        "        \"OOS R²\": r2_score(y_test, pred),\n",
        "        \"MSE\":    mean_squared_error(y_test, pred)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results).set_index(\"Method\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a57906",
      "metadata": {
        "id": "04a57906",
        "outputId": "1ab7dab5-589d-491a-c101-294b74fde708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           S.Avg OOS R²  TFIDF OOS R²  S.Avg MSE  TFIDF MSE\n",
            "Technique                                                  \n",
            "Diffusion      0.000752     -0.000593   0.000563   0.000563\n",
            "Laplacian     -0.000260     -0.000267   0.000563   0.000563\n",
            "PCA           -0.000509     -0.001027   0.000563   0.000564\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(results)\n",
        "\n",
        "# 1⃣  Split Method into Variant & Technique\n",
        "# We assume Method strings are \"<Variant> <Technique>\"\n",
        "df[['Variant', 'Technique']] = df['Method'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# 2⃣  Pivot so Technique is the index, Variant+Metric are columns\n",
        "pivot = df.pivot(index='Technique', columns='Variant', values=['OOS R²', 'MSE'])\n",
        "\n",
        "# 3⃣  Flatten and rename columns\n",
        "# This will produce a MultiIndex like ('OOS R²','Simple‑avg'), etc.\n",
        "# We want columns: ['S.Avg OOS R²','TFIDF OOS R²','S.Avg MSE','TFIDF MSE']\n",
        "col_map = {\n",
        "    'Simple‑avg': 'S.Avg',\n",
        "    'TF‑IDF':     'TFIDF'\n",
        "}\n",
        "\n",
        "new_cols = []\n",
        "for metric, variant in pivot.columns:\n",
        "    short = col_map.get(variant, variant)\n",
        "    new_cols.append(f\"{short} {metric}\")\n",
        "\n",
        "pivot.columns = new_cols\n",
        "\n",
        "# 4⃣  (Optional) order the columns as you like\n",
        "pivot = pivot[['S.Avg OOS R²', 'TFIDF OOS R²', 'S.Avg MSE', 'TFIDF MSE']]\n",
        "\n",
        "print(pivot)\n",
        "\n",
        "# Save the pivoted DataFrame to a CSV file\n",
        "pivot.to_csv('regression_results_TABLE.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4beecc68",
      "metadata": {
        "id": "4beecc68"
      },
      "outputs": [],
      "source": [
        "# # (Optional) assemble a single design DataFrame\n",
        "# # --------------------------------------------------\n",
        "# # If you want to run a single regression that includes all three sets of factors:\n",
        "# clean_all = (\n",
        "#     clean_df\n",
        "#     .join(clean_with_Z_pca[[f\"pca_{i}\" for i in range(1, 11)]])\n",
        "#     .join(clean_with_Z_lap[[f\"lap_{i}\" for i in range(1, 11)]])\n",
        "#     .join(clean_with_Z_diff[[f\"diff_{i}\" for i in range(1, 11)]])\n",
        "# )\n",
        "\n",
        "# # 4️⃣  (Optional) Combined regression\n",
        "# # -----------------------------------\n",
        "# Ztr_all = clean_all.loc[X_train.index,  pca_cols + lap_cols + diff_cols].values\n",
        "# Zte_all = clean_all.loc[X_test.index,   pca_cols + lap_cols + diff_cols].values\n",
        "\n",
        "# print(\"\\n=== Combined factors regression ===\")\n",
        "# eval_model(Ztr_all, Zte_all, y_train, y_test, \"All Factors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1b8ffb",
      "metadata": {
        "id": "dc1b8ffb"
      },
      "source": [
        "# OLS REGRESSIONS ON 10 PCs\n",
        "\n",
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OOS R²</th>\n",
        "      <th>MSE</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Method</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Laplacian</th>\n",
        "      <td>-0.000267</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Diffusion</th>\n",
        "      <td>-0.001862</td>\n",
        "      <td>0.000564</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>PCA</th>\n",
        "      <td>-0.002961</td>\n",
        "      <td>0.000565</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b0c7f9",
      "metadata": {
        "id": "79b0c7f9"
      },
      "source": [
        "# OLS REGRESSIONS ON 3 PCs\n",
        "\n",
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OOS R²</th>\n",
        "      <th>MSE</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Method</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Laplacian</th>\n",
        "      <td>-0.000267</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>PCA</th>\n",
        "      <td>-0.001027</td>\n",
        "      <td>0.000564</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Diffusion</th>\n",
        "      <td>-0.001124</td>\n",
        "      <td>0.000564</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b0a85e",
      "metadata": {
        "id": "65b0a85e"
      },
      "source": [
        "# RIDGE REGRESSIONS ON 3 PCs\n",
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OOS R²</th>\n",
        "      <th>MSE</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Method</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Laplacian</th>\n",
        "      <td>-0.000268</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Diffusion</th>\n",
        "      <td>-0.000631</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>PCA</th>\n",
        "      <td>-0.001026</td>\n",
        "      <td>0.000564</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb94fe0",
      "metadata": {
        "id": "1fb94fe0"
      },
      "source": [
        "# RIDGE REGRESSIONS ON 3 PCs w/ diffusion t = 5\n",
        "\n",
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OOS R²</th>\n",
        "      <th>MSE</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Method</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Laplacian</th>\n",
        "      <td>-0.000268</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Diffusion</th>\n",
        "      <td>-0.000631</td>\n",
        "      <td>0.000563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>PCA</th>\n",
        "      <td>-0.001026</td>\n",
        "      <td>0.000564</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c18c23",
      "metadata": {
        "id": "46c18c23"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yale_2025_qf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}