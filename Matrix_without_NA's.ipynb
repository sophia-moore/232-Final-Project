{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophia-moore/232-Final-Project/blob/main/Matrix_without_NA's.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "pLQ_3p529Hzz",
        "outputId": "93a5c0dc-7c73-47d3-c02e-237288f0831b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Date', 'Company', 'Title', 'Article', 'Compound_Return',\n",
            "       'Day1_Return'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf46a119b6bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# === 4. Load and filter your CSV embedding files ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0membed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/bag_of_words_doc_termcount_matrix.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Make sure the number of rows matches the metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1. Load the .pkl file ===\n",
        "meta_df = pd.read_pickle(\"/content/DJN_2017-01 (1).pkl\")\n",
        "\n",
        "# Check what columns are available\n",
        "print(meta_df.columns)\n",
        "\n",
        "# === 2. Filter to rows with valid (non-NaN) returns ===\n",
        "# Assuming the returns column is called \"returns\"\n",
        "valid_articles = meta_df.dropna(subset=[\"Day1_Return\"])\n",
        "\n",
        "# If needed: reset index to get row numbers\n",
        "valid_articles = valid_articles.reset_index(drop=True)\n",
        "\n",
        "# === 3. Save the valid row indices or IDs ===\n",
        "# Let's assume we use row positions\n",
        "valid_indices = valid_articles.index.tolist()\n",
        "\n",
        "# === 4. Load and filter your CSV embedding files ===\n",
        "embed_df = pd.read_csv(\"/content/bag_of_words_doc_termcount_matrix.csv\")\n",
        "\n",
        "# Make sure the number of rows matches the metadata\n",
        "assert len(embed_df) == len(meta_df), \"Mismatch between metadata and embeddings!\"\n",
        "\n",
        "# Filter using the valid indices\n",
        "filtered_embed_df = embed_df.iloc[valid_indices]\n",
        "\n",
        "# === 5. Save the cleaned embedding file ===\n",
        "filtered_embed_df.to_csv(\"bag_of_words_doc_termcount_matrix_clean\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the article metadata (contains a 'ticker' column)\n",
        "meta_df = pd.read_pickle(\"/content/DJN_2017-01 (1).pkl\")\n",
        "\n",
        "# Load the sector mapping (CSV with 'ticker' and 'sector' columns)\n",
        "sector_df = pd.read_csv(\"/content/nasdaq_screener_1746039118670.csv\")\n",
        "sector_df = sector_df[[\"Symbol\", \"Sector\"]]\n",
        "sector_df.columns = [\"Company\", \"Sector\"]\n",
        "\n",
        "# Check column names to confirm\n",
        "print(sector_df.columns)  # should show something like: ['ticker', 'sector']\n",
        "\n",
        "# Make sure both 'ticker' columns are strings\n",
        "meta_df[\"Company\"] = meta_df[\"Company\"].astype(str)\n",
        "sector_df[\"Company\"] = sector_df[\"Company\"].astype(str)\n",
        "\n",
        "# Step 1: Remove brackets from 'Company' (they are stored as lists)\n",
        "meta_df[\"Company\"] = meta_df[\"Company\"].str.strip(\"[]'\").str.upper()\n",
        "\n",
        "# Step 2: Make sure both are strings\n",
        "meta_df[\"Company\"] = meta_df[\"Company\"].astype(str).str.upper().str.strip()\n",
        "sector_df[\"Company\"] = sector_df[\"Company\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# Step 3: Merge on cleaned ticker names\n",
        "meta_df = meta_df.merge(sector_df, on=\"Company\", how=\"left\")\n",
        "\n",
        "meta_df.to_csv(\"OG_data_with_sector\", index=False)\n",
        "\n",
        "meta_df = meta_df.dropna(subset=[\"Day1_Return\"])\n",
        "\n",
        "# === 5. Save the cleaned embedding file ===\n",
        "meta_df.to_csv(\"OG_data_with_sector_without_NA\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PWtAv3DvI5",
        "outputId": "bfa0cf2e-e51a-41a9-c68e-2cd015398f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Company', 'Sector'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(meta_df.head(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-a2J16EFfuc",
        "outputId": "f04b2954-96aa-485f-d8a5-f7469adc0311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date    Company                                              Title  \\\n",
            "0   20170101     ATC.AE  [press, release, murphy, owner, kxly, abc, cha...   \n",
            "1   20170101       ARKR  [ark, restaurant, apos, ceo, weinstein, result...   \n",
            "2   20170101          F  [like, futurist, be, prepare, totally, unexpec...   \n",
            "3   20170101       NCOM  [press, release, national, commerce, corporati...   \n",
            "4   20170101        EDE  [press, release, district, electric, company, ...   \n",
            "..       ...        ...                                                ...   \n",
            "95  20170103     KER.FR  [press, release, half, yearly, achievement, re...   \n",
            "96  20170103        JPM  [indonesia, penalize, jpmorgan, underweight, r...   \n",
            "97  20170103    TILS.LN  [press, release, tiziana, life, science, in, l...   \n",
            "98  20170103         VZ  [press, release, link, labs, certifies, intern...   \n",
            "99  20170103  TBRAVO.XX  [private, equity, firm, clearlake, buy, securi...   \n",
            "\n",
            "                                              Article  Compound_Return  \\\n",
            "0   [remove, it, programming, wire, despite, willi...              NaN   \n",
            "1                              [from, seek, earnings]         0.070817   \n",
            "2   [by, in, resident, futurist, lead, team, imagi...         0.076860   \n",
            "3   [part, family, globe, ncc, parent, company, he...        -0.020161   \n",
            "4   [wire, closing, today, merger, company, subsid...              NaN   \n",
            "..                                                ...              ...   \n",
            "95  [half, yearly, wire, liquidity, mandate, grant...              NaN   \n",
            "96  [by, happens, bank, conduct, independent, rese...         0.003426   \n",
            "97  [anti, il, world, wide, license, expand, thera...              NaN   \n",
            "98  [wire, lead, innovator, low, power, wide, area...         0.023604   \n",
            "99  [by, in, one, first, deal, say, acquire, cyber...              NaN   \n",
            "\n",
            "    Day1_Return                  Sector  \n",
            "0           NaN                     NaN  \n",
            "1      0.008468  Consumer Discretionary  \n",
            "2      0.046068  Consumer Discretionary  \n",
            "3     -0.004098                     NaN  \n",
            "4           NaN                     NaN  \n",
            "..          ...                     ...  \n",
            "95          NaN                     NaN  \n",
            "96    -0.009205                 Finance  \n",
            "97          NaN                     NaN  \n",
            "98     0.002201      Telecommunications  \n",
            "99          NaN                     NaN  \n",
            "\n",
            "[100 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "meta_df = pd.read_csv(\"/content/OG_data_with_sector\")\n",
        "\n",
        "meta_df = meta_df.dropna(subset=[\"Day1_Return\"])\n",
        "\n",
        "# === 5. Save the cleaned embedding file ===\n",
        "meta_df.to_csv(\"OG_data_with_sector_without_NA\", index=False)\n"
      ],
      "metadata": {
        "id": "T61ES2mR2TR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}