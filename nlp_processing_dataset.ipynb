{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophia-moore/232-Final-Project/blob/main/nlp_processing_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OXUjiWVakrr"
      },
      "source": [
        "#### Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCCyCsbjakru"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyOWRsjSakrw",
        "outputId": "c0cefdd0-66ff-4b53-a4a2-5e683f59decc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gpapa\\AppData\\Local\\Temp\\ipykernel_2160\\3671339968.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  data = pickle.load(f)\n"
          ]
        }
      ],
      "source": [
        "# Read the pickle file\n",
        "file_path = r\"C:\\Users\\gpapa\\OneDrive\\My Life\\Education\\2025 YALE MMS\\Term_4S\\T4_E_Adv_Lin_Algebra_p2\\final_project\\DJN_2017-01.pkl\"\n",
        "with open(file_path, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka32sbeAakrx",
        "outputId": "fb75cb56-bc34-45fe-ce3a-b4c484a91c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "       Date   Company                                              Title  \\\n",
            "0  20170101  [ATC.AE]  [press, release, murphy, owner, kxly, abc, cha...   \n",
            "1  20170101    [ARKR]  [ark, restaurant, apos, ceo, weinstein, result...   \n",
            "2  20170101       [F]  [like, futurist, be, prepare, totally, unexpec...   \n",
            "3  20170101    [NCOM]  [press, release, national, commerce, corporati...   \n",
            "4  20170101     [EDE]  [press, release, district, electric, company, ...   \n",
            "\n",
            "                                             Article  Compound_Return  \\\n",
            "0  [remove, it, programming, wire, despite, willi...              NaN   \n",
            "1                             [from, seek, earnings]         0.070817   \n",
            "2  [by, in, resident, futurist, lead, team, imagi...         0.076860   \n",
            "3  [part, family, globe, ncc, parent, company, he...        -0.020161   \n",
            "4  [wire, closing, today, merger, company, subsid...              NaN   \n",
            "\n",
            "   Day1_Return  \n",
            "0          NaN  \n",
            "1     0.008468  \n",
            "2     0.046068  \n",
            "3    -0.004098  \n",
            "4          NaN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27413 entries, 0 to 27412\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Date             27413 non-null  object \n",
            " 1   Company          27413 non-null  object \n",
            " 2   Title            27413 non-null  object \n",
            " 3   Article          27413 non-null  object \n",
            " 4   Compound_Return  21534 non-null  float64\n",
            " 5   Day1_Return      21604 non-null  float64\n",
            "dtypes: float64(2), object(4)\n",
            "memory usage: 1.3+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Display basic information about the dataset\n",
        "print(type(data))\n",
        "if isinstance(data, pd.DataFrame):\n",
        "    print(data.head())\n",
        "    print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-tHn74nakrx",
        "outputId": "780bec49-0427-4b57-ddf7-ec847b5acf11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of titles: 9.08 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate average length of title lists\n",
        "avg_title_length_list = data['Title'].apply(len).mean()\n",
        "print(f\"Average length of titles: {avg_title_length_list:.2f} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXswY9u4akry",
        "outputId": "6f3d99e2-7321-4141-9ff7-62c7fed81c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of Articles: 210.09 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate average length of ARTICLE lists\n",
        "avg_article_length_list = data['Article'].apply(len).mean()\n",
        "print(f\"Average length of Articles: {avg_article_length_list:.2f} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXw1unvKakry"
      },
      "outputs": [],
      "source": [
        "unique_dates = data['Date'].value_counts()\n",
        "unique_dates.sort_index(ascending=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u88Cim-7akrz",
        "outputId": "ae101702-4892-447d-ddc1-0402b4ce40aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "20170101       6\n",
              "20170102      47\n",
              "20170103    1340\n",
              "20170104    2023\n",
              "20170105    1795\n",
              "20170106     988\n",
              "20170107      29\n",
              "20170108      42\n",
              "20170109    1136\n",
              "20170110    1260\n",
              "20170111    1102\n",
              "20170112    1185\n",
              "20170113     790\n",
              "20170114      20\n",
              "20170115      35\n",
              "20170116     348\n",
              "20170117    1376\n",
              "20170118    1402\n",
              "20170119    1510\n",
              "20170120     832\n",
              "20170121      14\n",
              "20170122      43\n",
              "20170123    1124\n",
              "20170124    1424\n",
              "20170125    1562\n",
              "20170126    1744\n",
              "20170127    1058\n",
              "20170128      21\n",
              "20170129      40\n",
              "20170130    1367\n",
              "20170131    1750\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_dates\n",
        "# our dates span from 2017-01-01 to 2017-01-31 => ONE MONTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73zIgS5Zakrz"
      },
      "source": [
        "#### Bag of Words (BoW) representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFhhEdXHakrz",
        "outputId": "fbd7d276-bae2-4e2d-c8f9-633d7864ba35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating document-term matrix...\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to strings in the Article column\n",
        "data['Article_text'] = data['Article'].apply(' '.join)\n",
        "\n",
        "# Step 1: Create document-term matrix\n",
        "print(\"Creating document-term matrix...\")\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.95,  # Remove terms that appear in >95% of documents\n",
        "    min_df=2,     # Remove terms that appear in <2 documents\n",
        "    stop_words='english'\n",
        ")\n",
        "doc_term_matrix = vectorizer.fit_transform(data['Article_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FLxwUboakr0",
        "outputId": "e498062b-a8a9-4682-f530-eb8c22f5f3c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document-term matrix created. Shape: (27413, 26617)\n",
            "Output – a SciPy sparse matrix shape = (n_docs, n_unique_terms) whose (i, j) entry is the raw count of term j in document i.\n"
          ]
        }
      ],
      "source": [
        "print(\"Document-term matrix created. Shape:\", doc_term_matrix.shape)\n",
        "print(\"Output – a SciPy sparse matrix shape = (n_docs, n_unique_terms) whose (i, j) entry is the raw count of term j in document i.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S9U1VSQakr0",
        "outputId": "fc003999-4e3f-45a4-f193-7578e573eccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 rows and 10 columns of the document-term matrix:\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Matrix shape: (27413, 26617)\n",
            "Number of non-zero elements: 2710110\n",
            "Sparsity: 99.63%\n"
          ]
        }
      ],
      "source": [
        "# Convert to dense array and show first few rows and columns\n",
        "print(\"First 10 rows and 10 columns of the document-term matrix:\")\n",
        "print(doc_term_matrix[:10, :10].toarray())\n",
        "\n",
        "# Show some basic statistics\n",
        "print(\"\\nMatrix shape:\", doc_term_matrix.shape)\n",
        "print(\"Number of non-zero elements:\", doc_term_matrix.nnz)\n",
        "print(\"Sparsity: {:.2%}\".format(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh2aCf0sakr0",
        "outputId": "6cbbc8a4-5654-4532-cc52-ef42349bb555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document-term matrix saved to bag_of_words_doc_termcount_matrix.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert sparse matrix to DataFrame\n",
        "df_bag_of_words = pd.DataFrame(\n",
        "\tdoc_term_matrix.toarray(),\n",
        "\tcolumns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_bag_of_words.to_csv('bag_of_words_doc_termcount_matrix.csv')\n",
        "print(\"Document-term matrix saved to bag_of_words_doc_termcount_matrix.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nyKBOXBakr1"
      },
      "source": [
        "#### Simple Average GloVe representation\n",
        "A bag-of-words matrix captures frequency but not semantics—“car” and “automobile” are orthogonal. Word-vector models like GloVe collapse words into dense real-valued vectors where geometric proximity encodes meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMD02xs8akr1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdRzJ1Drakr1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Document-Frequency counts -------------------------------------------\n",
        "N = len(data['Article'])                                # number of articles\n",
        "doc_freq_counts = Counter()\n",
        "\n",
        "for tokens in data['Article']:\n",
        "    doc_freq_counts.update(set(tokens))                       # set(tokens) removes duplicate tokens per doc\n",
        "\n",
        "# --- 2. Build the keep-set ---------------------------------------------------\n",
        "min_df = 2                                              # remove words that appear in <2 docs\n",
        "max_df = 0.95 * N                                       # remove words that appear in >95 % of all docs\n",
        "tokens_to_keep = {tok for tok, df in doc_freq_counts.items()\n",
        "                  if min_df <= df <= max_df}\n",
        "\n",
        "# --- 3. Filter each article --------------------------------------------------\n",
        "def filter_tokens(tokens, keep_set=tokens_to_keep):\n",
        "    return [t for t in tokens if t in keep_set]\n",
        "\n",
        "data['Article_winsorized'] = data['Article'].apply(filter_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai7h3BLDakr1"
      },
      "outputs": [],
      "source": [
        "# helper function to load GloVe embeddings\n",
        "def load_glove(path):\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9q3YcH7akr1",
        "outputId": "acae28c7-a0b6-4e61-d567-ad712de96ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe document embedding matrix shape: (27413, 100)\n",
            "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the simple avg embedding of characteristic in the i-th document.\n"
          ]
        }
      ],
      "source": [
        "# 1. Load pre-trained GloVe (you’ll need the txt file → dict{word:vector})\n",
        "glove = load_glove(r\"C:\\Users\\gpapa\\OneDrive\\My Life\\Education\\2025 YALE MMS\\Term_4S\\T4_E_Adv_Lin_Algebra_p2\\final_project\\glove.6B.100d.txt\")\n",
        "embedding_dim = 100\n",
        "UNK = np.zeros(embedding_dim)            # vector for out-of-vocabulary words\n",
        "\n",
        "def embed_article_avg(tokens):\n",
        "    vecs = [glove.get(tok, UNK) for tok in tokens]\n",
        "    return UNK if not vecs else np.mean(vecs, axis=0)         # simple average; or try tf-idf weighting\n",
        "\n",
        "# each Article_vec value is a emantic embedding of one article\n",
        "data['Article_vec'] = data['Article_winsorized'].apply(embed_article_avg)\n",
        "glove_doc_embedding_simple_avg_matrix = np.vstack(data['Article_vec'].values)\n",
        "\n",
        "print(\"GloVe document embedding matrix shape:\", glove_doc_embedding_simple_avg_matrix.shape)\n",
        "print(\"Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the simple avg embedding of characteristic (j) in document (i).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De_KId58akr1",
        "outputId": "210d764e-a045-4235-d84b-89b543c3a195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe document embedding matrix saved to glove_doc_embedding_simple_avg_matrix.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert glove embedding matrix to DataFrame\n",
        "df_glove_avg = pd.DataFrame(\n",
        "    glove_doc_embedding_simple_avg_matrix,\n",
        "    columns=[f'dim_{i}' for i in range(embedding_dim)]\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_glove_avg.to_csv('glove_doc_embedding_simple_avg_matrix.csv')\n",
        "print(\"GloVe document embedding matrix saved to glove_doc_embedding_simple_avg_matrix.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGl-ZSOqakr1",
        "outputId": "9719fc11-6ed3-446e-e3e0-2fb4c29b5f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows and 10 columns of the glove_doc_embedding_simple_avg_matrix:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.06428995,  0.03066079,  0.1261007 , -0.1087409 ,  0.06986396,\n",
              "        -0.2436581 , -0.14276559,  0.17012586, -0.0204917 ,  0.09696526],\n",
              "       [ 0.23832965,  0.16189666,  0.35848665, -0.09775668,  0.31639433,\n",
              "        -0.39629331, -0.3297292 , -0.15749334, -0.43084002, -0.07729667],\n",
              "       [-0.07722157,  0.23889745,  0.24618903, -0.04055617, -0.10148753,\n",
              "        -0.04471661, -0.12110174,  0.00650583,  0.02334156,  0.02951091],\n",
              "       [-0.05719619,  0.00981926,  0.16390821, -0.01661636,  0.01605625,\n",
              "        -0.17516939, -0.24395782,  0.00443692, -0.02972875,  0.01264667],\n",
              "       [-0.02815588,  0.04617887,  0.13838558,  0.07190471,  0.09651767,\n",
              "        -0.26731699, -0.14145903,  0.11651946, -0.139832  ,  0.03287467]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"First 5 rows and 10 columns of the glove_doc_embedding_simple_avg_matrix:\")\n",
        "display(glove_doc_embedding_simple_avg_matrix[:5, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT8gGvwzakr2"
      },
      "source": [
        "#### Full TF-IDF–Weighted GloVe Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8EBW-Wqakr2"
      },
      "outputs": [],
      "source": [
        "# --- 5. Precompute IDF values -----------------------------------------------\n",
        "idf = {}\n",
        "for token in tokens_to_keep:\n",
        "    df = doc_freq_counts[token]\n",
        "    idf[token] = np.log(1 + N / (1 + df))  # 1 + () -> smoothed IDF\n",
        "\n",
        "# --- 6. TF-IDF weighted document embedding ----------------------------------\n",
        "def embed_article_tfidf(tokens):\n",
        "    if not tokens:\n",
        "        return UNK\n",
        "\n",
        "    tf_counts = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "\n",
        "    tfidfs = []\n",
        "    tfidf_scaled_word_vecs = []\n",
        "\n",
        "    for tok in tokens:\n",
        "        vec = glove.get(tok, UNK)\n",
        "        tf_t = tf_counts[tok] / total_tokens\n",
        "        idf_val = idf.get(tok, 0.0)  # if not in IDF, weight is 0\n",
        "        tfidf = tf_t * idf_val\n",
        "        tfidfs.append(tfidf)\n",
        "        tfidf_scaled_word_vecs.append(vec * tfidf)\n",
        "\n",
        "    if sum(tfidfs) == 0:\n",
        "        return UNK\n",
        "\n",
        "    return np.sum(tfidf_scaled_word_vecs, axis=0) / sum(tfidfs)  # normalize word vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf4YtK6Dakr2",
        "outputId": "e9b3bd59-ec66-416b-a21e-2749e0bcc2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF weighted GloVe embedding matrix shape: (27413, 100)\n",
            "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the tfidf embedding of characteristic (j) in document (i)\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Apply to each article -----------------------------------------------\n",
        "data['Article_vec_tfidf'] = data['Article_winsorized'].apply(embed_article_tfidf)\n",
        "glove_doc_embedding_tfidf_matrix = np.vstack(data['Article_vec_tfidf'].values)\n",
        "\n",
        "# --- 8. Print output dimensions ---------------------------------------------\n",
        "print(\"TF-IDF weighted GloVe embedding matrix shape:\", glove_doc_embedding_tfidf_matrix.shape)\n",
        "print(\"Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the tfidf embedding of characteristic (j) in document (i)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRZXaydBakr2",
        "outputId": "9885a417-3902-4c0b-9bcc-415644912cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe document embedding matrix saved to glove_doc_embedding_tfidf_matrix.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert glove embedding matrix to DataFrame\n",
        "df_glove_tfidf = pd.DataFrame(\n",
        "    glove_doc_embedding_tfidf_matrix,\n",
        "    columns=[f'dim_{i}' for i in range(embedding_dim)]\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_glove_tfidf.to_csv('glove_doc_embedding_tfidf_matrix.csv')\n",
        "print(\"GloVe document embedding matrix saved to glove_doc_embedding_tfidf_matrix.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz0IG3aDakr2",
        "outputId": "4eaa4203-03a5-4c5a-a893-aee963149ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows and 10 columns of the glove_doc_embedding_tfidf_matrix:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.01056913, -0.1645497 ,  0.08856896, -0.19300572,  0.11462332,\n",
              "        -0.31052367, -0.06548257,  0.3515911 ,  0.02080597,  0.25400727],\n",
              "       [ 0.25694836,  0.16810628,  0.35881328, -0.09819148,  0.34195528,\n",
              "        -0.30055203, -0.34090286, -0.12114107, -0.3979063 , -0.04255791],\n",
              "       [-0.02555832,  0.32649209,  0.19000133, -0.03878729, -0.09883531,\n",
              "        -0.0310003 , -0.00478835, -0.19458799,  0.04868513,  0.1188786 ],\n",
              "       [ 0.00699697, -0.13084813,  0.17774754, -0.08311283, -0.01034015,\n",
              "        -0.20739683, -0.2062101 , -0.03980132, -0.04079613,  0.03588582],\n",
              "       [ 0.00395626, -0.04470648,  0.11056613,  0.0294033 ,  0.16738058,\n",
              "        -0.27139082, -0.09252032,  0.09953778, -0.19107341, -0.03167852]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"First 5 rows and 10 columns of the glove_doc_embedding_tfidf_matrix:\")\n",
        "display(glove_doc_embedding_tfidf_matrix[:5, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7EK770Qakr2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yale_2025_qf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}