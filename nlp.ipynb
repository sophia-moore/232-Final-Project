{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophia-moore/232-Final-Project/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qINm2lPzelsZ"
      },
      "source": [
        "#### Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7xF-9gQelsf"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Sequence, Dict, Tuple, Optional\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuEqoMG5elsg",
        "outputId": "f6ab8389-de95-440f-e9ea-b412e3d0ddc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gpapa\\AppData\\Local\\Temp\\ipykernel_25504\\3671339968.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  data = pickle.load(f)\n"
          ]
        }
      ],
      "source": [
        "# Read the pickle file\n",
        "file_path = r\"C:\\Users\\gpapa\\OneDrive\\My Life\\Education\\2025 YALE MMS\\Term_4S\\T4_E_Adv_Lin_Algebra_p2\\final_project\\DJN_2017-01.pkl\"\n",
        "with open(file_path, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMWE2UlNelsh",
        "outputId": "6a2d24b3-93c0-4d21-fb93-618a5fb52426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "       Date   Company                                              Title  \\\n",
            "0  20170101  [ATC.AE]  [press, release, murphy, owner, kxly, abc, cha...   \n",
            "1  20170101    [ARKR]  [ark, restaurant, apos, ceo, weinstein, result...   \n",
            "2  20170101       [F]  [like, futurist, be, prepare, totally, unexpec...   \n",
            "3  20170101    [NCOM]  [press, release, national, commerce, corporati...   \n",
            "4  20170101     [EDE]  [press, release, district, electric, company, ...   \n",
            "\n",
            "                                             Article  Compound_Return  \\\n",
            "0  [remove, it, programming, wire, despite, willi...              NaN   \n",
            "1                             [from, seek, earnings]         0.070817   \n",
            "2  [by, in, resident, futurist, lead, team, imagi...         0.076860   \n",
            "3  [part, family, globe, ncc, parent, company, he...        -0.020161   \n",
            "4  [wire, closing, today, merger, company, subsid...              NaN   \n",
            "\n",
            "   Day1_Return  \n",
            "0          NaN  \n",
            "1     0.008468  \n",
            "2     0.046068  \n",
            "3    -0.004098  \n",
            "4          NaN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27413 entries, 0 to 27412\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Date             27413 non-null  object \n",
            " 1   Company          27413 non-null  object \n",
            " 2   Title            27413 non-null  object \n",
            " 3   Article          27413 non-null  object \n",
            " 4   Compound_Return  21534 non-null  float64\n",
            " 5   Day1_Return      21604 non-null  float64\n",
            "dtypes: float64(2), object(4)\n",
            "memory usage: 1.3+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Display basic information about the dataset\n",
        "print(type(data))\n",
        "if isinstance(data, pd.DataFrame):\n",
        "    print(data.head())\n",
        "    print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqiNgYbHelsh",
        "outputId": "0755f1e9-9072-4806-8a90-372c7b2d4bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of titles: 9.08 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate average length of title lists\n",
        "avg_title_length_list = data['Title'].apply(len).mean()\n",
        "print(f\"Average length of titles: {avg_title_length_list:.2f} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSpf5wnaelsh",
        "outputId": "6238388d-96d3-4e68-b0f9-ebfa2c12ca6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of Articles: 210.09 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate average length of ARTICLE lists\n",
        "avg_article_length_list = data['Article'].apply(len).mean()\n",
        "print(f\"Average length of Articles: {avg_article_length_list:.2f} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usHQPLaLelsh"
      },
      "outputs": [],
      "source": [
        "unique_dates = data['Date'].value_counts()\n",
        "unique_dates.sort_index(ascending=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je2D-P9Celsh",
        "outputId": "53081ed0-bf72-42b3-e900-2fdaa0113e23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "20170101       6\n",
              "20170102      47\n",
              "20170103    1340\n",
              "20170104    2023\n",
              "20170105    1795\n",
              "20170106     988\n",
              "20170107      29\n",
              "20170108      42\n",
              "20170109    1136\n",
              "20170110    1260\n",
              "20170111    1102\n",
              "20170112    1185\n",
              "20170113     790\n",
              "20170114      20\n",
              "20170115      35\n",
              "20170116     348\n",
              "20170117    1376\n",
              "20170118    1402\n",
              "20170119    1510\n",
              "20170120     832\n",
              "20170121      14\n",
              "20170122      43\n",
              "20170123    1124\n",
              "20170124    1424\n",
              "20170125    1562\n",
              "20170126    1744\n",
              "20170127    1058\n",
              "20170128      21\n",
              "20170129      40\n",
              "20170130    1367\n",
              "20170131    1750\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_dates\n",
        "# our dates span from 2017-01-01 to 2017-01-31 => ONE MONTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoR6VdiFelsh",
        "outputId": "6d7ca647-bacb-4ce7-a3b7-b6d1a07a2889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows kept (non-duplicate articles): 11,171\n",
            "Train size               : 8,378\n",
            "Test  size               : 2,793\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# df = pd.read_csv(\"path/to/your/dataset.csv\")   # <- uncomment & adapt if you haven’t loaded it yet\n",
        "assert {'Article', 'Day1_Return'}.issubset(data.columns), \"Missing one of the required columns.\"\n",
        "\n",
        "# 3. Keep only observations with a target value\n",
        "clean_df = data.dropna(subset=['Day1_Return']).reset_index(drop=True)\n",
        "\n",
        "base = clean_df.drop_duplicates(subset='Article')\n",
        "\n",
        "# 4-A.  IID (shuffled) 80 / 20 split  -------------------------\n",
        "#      (Most common choice for cross-sectional text problems)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    base['Article'],             # features (raw tokens for now)\n",
        "    base['Day1_Return'],         # target\n",
        "    test_size=0.25,                  # 25 % hold-out\n",
        "    random_state=42,                 # reproducible shuffling\n",
        "    shuffle=True                     # default; keeps rows IID\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4-B.  Time-ordered split  (uncomment if you prefer it)\n",
        "#      Keeps entire “future” days completely out-of-sample.\n",
        "# ------------------------------------------------------------\n",
        "# clean_df['Date'] = pd.to_datetime(clean_df['Date'].astype(str))  # yyyymmdd → Timestamp\n",
        "# clean_df = clean_df.sort_values('Date').reset_index(drop=True)\n",
        "# split_idx   = int(0.80 * len(clean_df))\n",
        "# X_train     = clean_df.loc[:split_idx-1, 'Article']\n",
        "# y_train     = clean_df.loc[:split_idx-1, 'Day1_Return']\n",
        "# X_test      = clean_df.loc[split_idx:,   'Article']\n",
        "# y_test      = clean_df.loc[split_idx:,   'Day1_Return']\n",
        "\n",
        "# 5.  Quick sanity check\n",
        "print(f\"Rows kept (non-duplicate articles): {len(base):,}\")\n",
        "print(f\"Train size               : {len(X_train):,}\")\n",
        "print(f\"Test  size               : {len(X_test):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NZ6mB7Zelsi"
      },
      "source": [
        "**Rows kept with deduplicates (non-null target): 21,604  \n",
        "Train size               : 16,203  \n",
        "Test  size               : 5,401**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlqioYQFelsi",
        "outputId": "c9c280b3-c46f-4405-8956-409aea565a14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Article\n",
              "[insider, surrender, share, back, company, pay, tax, cover, cost, option, exercise, file, insider, transaction, company, apos, share, open, market, purchase, sale, must, report, within, two, business, day, transaction, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1799\n",
              "[form, file, reflect, intention, holder, restrict, stock, share, after, mail, filer, permit, share, fraction, within, day, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1552\n",
              "[end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1207\n",
              "[direct, insider, surrender, share, back, company, pay, tax, cover, cost, option, exercise, file, insider, transaction, company, apos, share, open, market, purchase, sale, must, report, within, two, business, day, transaction, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           745\n",
              "[rating, action]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  709\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ... \n",
              "[pointe, claire, wire, voluntarily, recall, certain, smokeless, tobacco, product, follow, similar, action, take, initiated, recall, receive, eight, consumer, complaint, foreign, metal, object, include, sharp, metal, object, find, select, can, in, case, object, visible, consumer, report, consumer, injury, complaint, receive, consumer, aware, complaint, individual, product, issue, manufacture, facility, also, manufacture, smokeless, tobacco, product, recall, certain, lot, product, variety, describe, recall, instruct, retailer, wholesaler, segregate, recall, product, inventory, assist, retailer, wholesaler, return, product, consumer, product, list, table, product, check, coming, day, additional, information, select, can, subject, recall, this, recall, apply, can, print, code, bottom, lot, one, follow, date, code, bottom, this, recall, ...]                                                    1\n",
              "[company, announces, syndicate, credit, facility, mature, remain, failure, repay, approximate, million, principal, amount, owe, constitutes, event, default, entitle, lender, among, thing, enforce, security, appoint, receiver, manage, affair, an, event, default, also, event, default, credit, facility, wholly, own, subsidiary, previously, disclose, certain, financial, non, financial, covenant, breach, year, end, interim, financial, statement, although, lender, enact, remedy, relation, current, event, default, past, covenant, breach, reserve, available, right, remedy, respect, thereof, currently, negotiation, lender, extension, allow, continue, strategic, process, however, certainty, whether, extension, obtain, lender, term, previously, announce, initiated, process, strategic, alternative, maximize, value, significant, light, oil, base, company, update, time, continue, evaluate, ...]       1\n",
              "[prnewswire, announce, today, pricing, initial, offering, share, common, stock, price, per, share, grant, underwriter, day, option, purchase, additional, share, common, stock, initial, offering, price, less, underwriting, discount, commission, common, stock, expect, begin, trade, symbol, laur, offering, expect, close, subject, customary, closing, condition, act, lead, book, run, manager, representative, underwriter, act, book, run, manager, offer, act, co, manager, offer, registration, statement, include, prospectus, relate, offer, share, class, common, stock, declare, effective, offering, make, mean, prospectus, copy, final, prospectus, related, offering, may, obtain, call, free, email, newyorkprospectus, nd, call, free, email, barclaysprospectus, this, press, release, constitute, offer, solicitation, offer, buy, sale, security, ...]                                                      1\n",
              "[cobaltech, announces, approve, grant, stock, option, director, officer, employee, consultant, pursuant, approve, stock, option, plan, option, grant, exercisable, price, per, share, term, five, year, date, grant, subject, regulatory, approval, in, keep, policy, stock, option, plan, option, subject, four, month, hold, period, about, cobalt, mining, process, company, base, own, operates, locate, outside, area, geological, setting, responsible, mineralization, compose, carbonate, vein, enrich, silver, cobalt, bismuth, arsenic, asset, technology, provide, become, first, vertically, integrated, cobalt, process, company, capacity, take, mineralize, ore, production, metal, lithium, battery, industry, commit, operating, within, environmental, health, safety, framework, govern, plus, regulation, put, forth, of, term, define, policy, accept, ...]                                                    1\n",
              "[plan, rollout, robo, advisor, hop, technology, help, snare, intergenerational, money, motion, so, reports, firm, pilot, advice, quarter, roll, plus, house, independent, broker, client, option, interact, directly, still, access, financial, advisor, but, want, access, robo, need, financial, advisor, accord, firm, aposs, technology, operation, stay, true, adviser, base, method, firm, offer, five, different, portfolio, mutual, fund, base, much, risk, investor, willing, take, active, passive, investing, strategy, available, expect, robo, important, tool, advisor, age, client, child, line, inherit, wealth, other, brokerage, roll, advice, service, announce, plan, recent, month, launch, last, month, and, announce, partnership, develop, similar]                                                                                                                                                         1\n",
              "Name: count, Length: 15728, dtype: int64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dirty_articles_counts = data['Article'].value_counts()\n",
        "dirty_articles_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p79TmWRtelsi",
        "outputId": "1ecb8b00-a649-4076-d5fd-6f78521e4d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate articles: 10,433\n"
          ]
        }
      ],
      "source": [
        "#check how many duplicate values in Article column are there\n",
        "duplicates = clean_df['Article'].duplicated().sum()\n",
        "print(f\"Number of duplicate articles: {duplicates:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cujNnXLselsi",
        "outputId": "1584a7ba-5246-4af9-f3de-cbf73b7953ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique articles: 11,171\n",
            "\n",
            "\n",
            "Number of unique articles that have duplicates: 421\n"
          ]
        }
      ],
      "source": [
        "# Count unique articles that have duplicates\n",
        "article_counts = clean_df['Article'].value_counts()\n",
        "print(f\"Number of unique articles: {len(article_counts):,}\")\n",
        "print('\\n')\n",
        "different_duplicates = len(article_counts[article_counts > 1])\n",
        "print(f\"Number of unique articles that have duplicates: {different_duplicates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki3o3yb4elsi",
        "outputId": "da615ad8-b979-4268-f276-2051eac84661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Article\n",
              "[insider, surrender, share, back, company, pay, tax, cover, cost, option, exercise, file, insider, transaction, company, apos, share, open, market, purchase, sale, must, report, within, two, business, day, transaction, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1785\n",
              "[form, file, reflect, intention, holder, restrict, stock, share, after, mail, filer, permit, share, fraction, within, day, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1515\n",
              "[end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         870\n",
              "[direct, insider, surrender, share, back, company, pay, tax, cover, cost, option, exercise, file, insider, transaction, company, apos, share, open, market, purchase, sale, must, report, within, two, business, day, transaction, end]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       744\n",
              "[rating, action]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              707\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ... \n",
              "[participate, globe, today, announce, management, schedule, participate, schedule, participate, panel, nd, pst, participation, invitation, registration, require, information, conference, schedule, one, one, meeting, management, please, call, bank, hold, company, establish, parent, company, establish, headquartered, full, branch, subsidiary, base, provide, business, essential, work, capital, factor, financing, various, industry, throughout, information, please]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "[three, nominated, wire, nyse, arnc, today, confirm, unanimously, support, in, response, press, release, issue, issue, follow, statement, unanimous, support, in, addition, consist, director, independent, substantially, reconstitute, new, director, independent, perspective, independent, director, join, within, last, year, nominate, join, appoint, connection, separation, all, director, executive, former, executive, broad, range, experience, skill, high, level, private, company, conduct, intensive, allegation, conclude, many, misleading, substantiate, welcome, shareholder, input, dialogue, shareholder, endeavor, work, constructively, believe, continued, effort, disruptive, contrary, best, interest, shareholder, fully, engage, focused, oversee, execution, strategic, plan, leadership, management, team, focus, continue, improve, operating, result, expand, margin, improve, return, net, asset, deliver, sustain, ...]       1\n",
              "[by, say, plan, build, first, air, cargo, hub, accommodate, grow, fleet, plane, signal, company, ramp, expansion, transport, sort, deliver, package, seattle, base, say, expect, new, air, hub, locate, create, job, move, lessen, dependence, traditional, carrier, include, whose, large, hub, nearby, last, year, say, planning, cargo, plane, currently, fleet, it, also, bring, dedicate, network, semi, trailer, increase, truck, capacity, fleet, citizen, courier, driver, make, delivery, major, metro, area, goal, eventually, haul, deliver, package, well, retailer, consumer, make, direct, competitor, accord, people, familiar, matter, air, cargo, hub, follow, recent, ocean, debut, handle, shipment, good, ocean, warehouse, merchant, sell, site, take, role, previously, ...]                                                                                                                                                              1\n",
              "[prnewswire, investigate, potential, claim, board, director, roadrunner, company, nyse, our, investigation, concern, potential, breach, fiduciary, duty, security, claim, investigation, stem, announcement, certain, financial, statement, longer, rely, in, filing, announce, make, aware, various, potential, accounting, discrepancy, operate, subsidiary, result, investigation, currently, estimate, require, prior, period, adjustment, result, operation, million, million, annual, quarterly, financial, reporting, year, accord, error, principally, relate, unrecorded, expense, unreconciled, balance, sheet, account, include, cash, driver, linehaul, driver, payable, reassess, internal, control, financial, report, compliance, program, on, news, share, fell, sharply, hour, trade, if, share, like, learn, class, action, wish, discus, matter, question, concern, announcement, right, contact, free, sign, ...]                           1\n",
              "[information, and, prnewswire, today, announce, tax, reporting, div, information, distribution, common, share, preferred, share, income, tax, treatment, trade, ticker, symbol, ahp, follow, income, tax, treatment, distribution, follow, common, preferred, distribution, company, pay, shareholder, record, reportable, common, preferred, distribution, company, pay, shareholder, record, reportable, company, encourage, shareholder, consult, tax, advisor, respect, federal, state, local, foreign, income, tax, effect, dividend, real, estate, investment, focus, investing, luxury, hotel, resort, create, hospitality, investor, community, available, free, download, search, statement, assumption, press, release, contain, base, look, information, make, pursuant, safe, harbor, provision, these, look, statement, subject, risk, uncertainty, when, word, likely, result, may, similar, expression, ...]                                     1\n",
              "Name: count, Length: 11171, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_df['Article'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwicS2Grelsi",
        "outputId": "092d3e0d-91e7-4109-be13-d47cf12b0e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Details for the most frequent article:\n",
            "           Date Company  Day1_Return\n",
            "90     20170103  [TMUS]    -0.020571\n",
            "334    20170103   [QTS]     0.002887\n",
            "343    20170103   [QTS]     0.002887\n",
            "352    20170103   [QTS]     0.002887\n",
            "376    20170103   [QTS]     0.002887\n",
            "...         ...     ...          ...\n",
            "21548  20170131   [AMN]     0.024793\n",
            "21561  20170131  [VSAR]     0.040590\n",
            "21563  20170131  [VSAR]     0.040590\n",
            "21566  20170131  [VSAR]     0.040590\n",
            "21567  20170131  [AMGN]     0.049818\n",
            "\n",
            "[1785 rows x 3 columns]\n",
            "\n",
            "This article appears 1785 times in the dataset\n",
            "\n",
            "Article content (tokens):\n",
            "['insider', 'surrender', 'share', 'back', 'company', 'pay', 'tax', 'cover', 'cost', 'option', 'exercise', 'file', 'insider', 'transaction', 'company', 'apos', 'share', 'open', 'market', 'purchase', 'sale', 'must', 'report', 'within', 'two', 'business', 'day', 'transaction', 'end']\n"
          ]
        }
      ],
      "source": [
        "# Get the most common article and its details\n",
        "most_common_article = clean_df['Article'].value_counts().index[0]\n",
        "\n",
        "# Convert the lists to tuples for comparison (lists aren't hashable)\n",
        "matching_mask = clean_df['Article'].apply(lambda x: tuple(x) == tuple(most_common_article))\n",
        "matching_rows = clean_df[matching_mask][['Date', 'Company', 'Day1_Return']]\n",
        "\n",
        "print(\"\\nDetails for the most frequent article:\")\n",
        "print(matching_rows)\n",
        "print(f\"\\nThis article appears {len(matching_rows)} times in the dataset\")\n",
        "\n",
        "# Optional: Print the actual article content (as a list of tokens)\n",
        "print(\"\\nArticle content (tokens):\")\n",
        "print(most_common_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-wkZBQvelsj",
        "outputId": "f8fb3e2b-4475-40b1-db4e-3a94dc024afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique articles: 11,171\n",
            "\n",
            "\n",
            "Number of unique articles that have duplicates: 0\n"
          ]
        }
      ],
      "source": [
        "# Count unique articles that have duplicates\n",
        "base_counts = base['Article'].value_counts()\n",
        "print(f\"Number of unique articles: {len(base_counts):,}\")\n",
        "print('\\n')\n",
        "different_base_duplicates = len(base_counts[base_counts > 1])\n",
        "print(f\"Number of unique articles that have duplicates: {different_base_duplicates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67XtlA2Zelsj",
        "outputId": "33dc8611-ac7c-4dd3-8d8d-9a3ad7ea68ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2083     [operate, direct, insider, surrender, share, b...\n",
              "10443    [more, follow, press, release, market, drive, ...\n",
              "18941    [investigate, nyse, so, globe, today, announce...\n",
              "3578     [salesforcecom, inc, form, file, reflect, inte...\n",
              "3203     [wire, today, announce, former, member, member...\n",
              "1464     [it, globe, today, announce, joined, company, ...\n",
              "15633    [prnewswire, leader, ownership, development, m...\n",
              "61       [unaudited, fiscal, year, prnewswire, signific...\n",
              "8913     [wire, zayo, direct, subsidiary, global, provi...\n",
              "11011    [help, plant, produce, low, carbon, support, g...\n",
              "Name: Article, dtype: object"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqMatKb6elsj",
        "outputId": "14642558-1be5-4da6-fc25-7fcccab96baa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7637     [by, leader, big, pharma, aposs, cancer, race,...\n",
              "16822                   [follow, press, release, in, with]\n",
              "18162    [by, after, market, close, schedule, announce,...\n",
              "10523    [announces, tax, treatment, distribution, prne...\n",
              "5812     [azure, base, help, wire, in, grow, migration,...\n",
              "118      [globe, nasdaq, apri, biopharmaceutical, compa...\n",
              "8502     [by, say, online, plan, create, full, time, jo...\n",
              "21030    [more, more, more, second, globe, nasdaq, ktcc...\n",
              "982      [globe, nasdaq, xog, oil, gas, exploration, pr...\n",
              "21355    [put, little, extra, pressure, say, aposs, mak...\n",
              "Name: Article, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLNR_xPUelsj"
      },
      "source": [
        "#### Bag of Words (BoW) representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG_GYBnGelsj"
      },
      "outputs": [],
      "source": [
        "def build_bow_matrix(\n",
        "    articles: pd.Series,\n",
        "    vectorizer: Optional[CountVectorizer] = None,\n",
        "    *,\n",
        "    max_df: float = 0.95,\n",
        "    min_df: int = 2,\n",
        "    stop_words: str = \"english\"\n",
        ") -> Tuple:\n",
        "    \"\"\"\n",
        "    Convert a Series of token-lists into a Bag-of-Words matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    articles   : pd.Series\n",
        "        Each element is a list of tokens (words) for one document.\n",
        "    vectorizer : CountVectorizer or None\n",
        "        • None  → fit a new CountVectorizer on `articles` and return it.\n",
        "        • existing CountVectorizer → skip fitting and only transform.\n",
        "    max_df, min_df, stop_words\n",
        "        Passed straight to CountVectorizer (ignored if a pre-fitted\n",
        "        `vectorizer` is supplied).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X          : scipy sparse matrix  (n_docs × |V|)\n",
        "    vectorizer : the fitted CountVectorizer (use for future transforms)\n",
        "    \"\"\"\n",
        "    # 1  join each token list into a single space-separated string\n",
        "    article_text = articles.apply(\" \".join)\n",
        "\n",
        "    # 2  fit or transform\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words)\n",
        "        X = vectorizer.fit_transform(article_text)\n",
        "    else:\n",
        "        X = vectorizer.transform(article_text)\n",
        "\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LmdHMxDelsj"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# X_train, X_test are the token-list Series\n",
        "# produced earlier by the train_test_split step\n",
        "# ---------------------------------------------\n",
        "\n",
        "# Fit on training data\n",
        "X_train_bow, bow_vect = build_bow_matrix(X_train)\n",
        "\n",
        "# Transform held-out test data with the *same* vocabulary\n",
        "X_test_bow,  _        = build_bow_matrix(X_test, vectorizer=bow_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCQN8ivselsk",
        "outputId": "38b15163-9bab-457f-b6d3-89d2720e11fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Document-term matrix created. Shape: (8378, 18033)\n",
            "Test Document-term matrix created. Shape: (2793, 18033)\n",
            "Output – a SciPy sparse matrix shape = (n_docs, n_unique_train_set_terms) whose (i, j) entry is the raw count of term j in document i.\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Document-term matrix created. Shape:\", X_train_bow.shape)\n",
        "print(\"Test Document-term matrix created. Shape:\", X_test_bow.shape)\n",
        "print(\"Output – a SciPy sparse matrix shape = (n_docs, n_unique_train_set_terms) whose (i, j) entry is the raw count of term j in document i.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McqKuyC2elsk",
        "outputId": "f1d57ff5-f35a-497c-dd8b-60459a77cf80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document-term matrix saved to bow_train_matrix.csv\n",
            "Document-term matrix saved to bow_test_matrix.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert sparse matrix to DataFrame\n",
        "df_train_bow = pd.DataFrame(\n",
        "\tX_train_bow.toarray(),\n",
        "\tcolumns=bow_vect.get_feature_names_out()\n",
        ")\n",
        "\n",
        "df_test_bow = pd.DataFrame(\n",
        "\tX_test_bow.toarray(),\n",
        "\tcolumns=bow_vect.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_train_bow.to_csv('bow_train_matrix.csv')\n",
        "print(\"Document-term matrix saved to bow_train_matrix.csv\")\n",
        "\n",
        "# Save to CSV\n",
        "df_test_bow.to_csv('bow_test_matrix.csv')\n",
        "print(\"Document-term matrix saved to bow_test_matrix.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFGlTOD1elsk"
      },
      "source": [
        "#### Simple Average & TF-IDF GloVe representation\n",
        "A bag-of-words matrix captures frequency but not semantics—“car” and “automobile” are orthogonal. Word-vector models like GloVe collapse words into dense real-valued vectors where geometric proximity encodes meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj7wZAJ4elsk"
      },
      "outputs": [],
      "source": [
        "# 0 Load pre-trained GloVe once (100-d here — change path if needed)\n",
        "# ------------------------------------------------------------------\n",
        "def load_glove(path: str) -> Dict[str, np.ndarray]:\n",
        "    out = {}\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split()\n",
        "            out[parts[0]] = np.asarray(parts[1:], dtype=np.float32)\n",
        "    return out\n",
        "\n",
        "GLOVE_PATH   = r\"C:\\Users\\gpapa\\OneDrive\\My Life\\Education\\2025 YALE MMS\\Term_4S\\T4_E_Adv_Lin_Algebra_p2\\final_project\\GloVe Embeddings\\glove.6B.100d.txt\"\n",
        "glove        = load_glove(GLOVE_PATH)\n",
        "EMBED_DIM    = 100\n",
        "UNK          = np.zeros(EMBED_DIM, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIAqPYVzelsk"
      },
      "outputs": [],
      "source": [
        "# 1 Fit document-frequency stats **on training only**\n",
        "# ------------------------------------------------------------------\n",
        "def fit_vocab_and_idf(\n",
        "    docs: Sequence[Sequence[str]],\n",
        "    min_df: int   = 2,\n",
        "    max_df_ratio: float = 0.95\n",
        ") -> Tuple[set, Dict[str, float]]:\n",
        "    \"\"\"Return {kept tokens}, and IDF dict computed on `docs`.\"\"\"\n",
        "    N = len(docs)\n",
        "    df_counts = Counter()\n",
        "    for tokens in docs:\n",
        "        df_counts.update(set(tokens))\n",
        "\n",
        "    keep = {tok for tok, df in df_counts.items()\n",
        "            if min_df <= df <= max_df_ratio * N}\n",
        "\n",
        "    idf  = {tok: np.log(1 + N / (1 + df_counts[tok])) for tok in keep}\n",
        "    return keep, idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HRGphvjelsk"
      },
      "outputs": [],
      "source": [
        "# 2 Embed a list-of-tokens article  (avg  or  tf-idf weighted)\n",
        "# ------------------------------------------------------------------\n",
        "def embed_doc(tokens: Sequence[str],\n",
        "              *,\n",
        "              kind: str,                 # \"avg\"  or  \"tfidf\"\n",
        "              keep: set,\n",
        "              idf: Dict[str, float] = None\n",
        ") -> np.ndarray:\n",
        "    toks = [t for t in tokens if t in keep]\n",
        "    if not toks:\n",
        "        return UNK\n",
        "\n",
        "    if kind == \"avg\":\n",
        "        return np.mean([glove.get(t, UNK) for t in toks], axis=0)\n",
        "\n",
        "    if kind == \"tfidf\":\n",
        "        tf = Counter(toks)\n",
        "        total_doc_tokens = len(toks)\n",
        "        weights, scaled_word_vecs = [], []\n",
        "        for t in toks:\n",
        "            weight = (tf[t] / total_doc_tokens) * idf.get(t, 0.0)\n",
        "            weights.append(weight)\n",
        "            scaled_word_vecs.append(glove.get(t, UNK) * weight)\n",
        "        w_sum = sum(weights)\n",
        "        return UNK if w_sum == 0 else np.sum(scaled_word_vecs, axis=0) / w_sum\n",
        "\n",
        "    raise ValueError(\"kind must be 'avg' or 'tfidf'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKSA5eAjelsk"
      },
      "outputs": [],
      "source": [
        "# 3 Pipeline – from tokens to four matrices\n",
        "# ------------------------------------------------------------------\n",
        "def build_glove_matrices(\n",
        "    X_train: pd.Series,    # Series of token lists\n",
        "    X_test:  pd.Series\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "\n",
        "    keep, idf = fit_vocab_and_idf(X_train)\n",
        "\n",
        "    # helper lambdas avoid repeated kwargs\n",
        "    avg   = lambda toks:   embed_doc(toks, kind=\"avg\",   keep=keep)\n",
        "    tfidf = lambda toks:   embed_doc(toks, kind=\"tfidf\", keep=keep, idf=idf)\n",
        "\n",
        "    Xtr_avg  = np.vstack(X_train.apply(avg).values)\n",
        "    Xte_avg  = np.vstack(X_test.apply(avg).values)\n",
        "\n",
        "    Xtr_tf   = np.vstack(X_train.apply(tfidf).values)\n",
        "    Xte_tf   = np.vstack(X_test.apply(tfidf).values)\n",
        "\n",
        "    return Xtr_avg, Xte_avg, Xtr_tf, Xte_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4If2mxcelsk",
        "outputId": "4d19d18f-9821-43c9-c6cd-6934ef24e791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes  –  Train avg: (8378, 100) Test avg: (2793, 100)\n",
            "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the simple avg embedding of characteristic (j) in document (i).\n",
            "\n",
            "\n",
            "Shapes  – Train tfidf: (8378, 100) Test tfidf: (2793, 100)\n",
            "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the tfidf embedding of characteristic (j) in document (i)\n"
          ]
        }
      ],
      "source": [
        "# 4 Run it\n",
        "# ------------------------------------------------------------------\n",
        "X_train_glove_avg,  X_test_glove_avg, \\\n",
        "X_train_glove_tfidf, X_test_glove_tfidf = build_glove_matrices(X_train, X_test)\n",
        "\n",
        "print(\"Shapes  –  Train avg:\",   X_train_glove_avg.shape, \"Test avg:\",  X_test_glove_avg.shape)\n",
        "print(\"Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the simple avg embedding of characteristic (j) in document (i).\")\n",
        "print('\\n')\n",
        "print(\"Shapes  – Train tfidf:\", X_train_glove_tfidf.shape, \"Test tfidf:\", X_test_glove_tfidf.shape)\n",
        "print(\"Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the tfidf embedding of characteristic (j) in document (i)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QHDNQf3elsl"
      },
      "source": [
        "**Shapes  –  Duplicates Train avg: (16203, 100) Duplicates Test avg: (5401, 100)  \n",
        "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the simple avg embedding of characteristic (j) in document (i).**   \n",
        "\n",
        "\n",
        "**Shapes  – Duplicates Train tfidf: (16203, 100) Duplicates Test tfidf: (5401, 100)  \n",
        "Output – a NumPy array shape = (n_docs, n_embedding_dim) whose (i,j) entry is the tfidf embedding of characteristic (j) in document (i)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoQ5-hDMelsl",
        "outputId": "29f9597b-1f67-4afd-d5ff-f0f5606ed589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows and 10 columns of the train avg matrix:\n",
            "[[ 0.07170191  0.06997532  0.16436417 -0.02037911  0.08340277]\n",
            " [-0.01395935  0.05536129  0.26111835  0.01325093  0.00734193]\n",
            " [ 0.03635607  0.02148002  0.19563235 -0.02306763  0.01742517]\n",
            " [ 0.07147374  0.12722564  0.18284309 -0.01860794  0.11183619]\n",
            " [-0.04374495  0.05594995  0.12998235  0.01704939  0.05162265]]\n",
            "\n",
            "\n",
            "First 5 rows and 10 columns of the train tfidf matrix:\n",
            "[[ 0.16753436 -0.04237869  0.24857812 -0.01005682  0.14047462]\n",
            " [ 0.07926302  0.04166919  0.35364992 -0.09686092 -0.06041234]\n",
            " [ 0.03443183 -0.0550491   0.20656885 -0.02302127  0.00702674]\n",
            " [ 0.06877818  0.08092811  0.14684223 -0.1043917   0.18785022]\n",
            " [ 0.00657955  0.04952998 -0.0248589   0.02433235  0.07496377]]\n"
          ]
        }
      ],
      "source": [
        "# 5 Display\n",
        "# -----------------------------------------------------------------\n",
        "print(\"First 5 rows and 10 columns of the train avg matrix:\")\n",
        "print(X_train_glove_avg[:5, :5])\n",
        "print('\\n')\n",
        "print(\"First 5 rows and 10 columns of the train tfidf matrix:\")\n",
        "print(X_train_glove_tfidf[:5, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCj3fRrMelsl"
      },
      "source": [
        "First 5 rows and 5 columns of the train avg matrix:  \n",
        "[[ 0.06743193  0.08372917  0.19575463 -0.04276435  0.01124586]  \n",
        " [ 0.08235665  0.08270617  0.19840498 -0.02022048  0.06879219]  \n",
        " [-0.2032155   0.22544767  0.14584434  0.09497339  0.101927  ]  \n",
        " [ 0.14726     0.088411    0.28386    -0.2797     -0.32498   ]  \n",
        " [ 0.03238507  0.06798428  0.22245008 -0.03258403  0.1515859 ]]  \n",
        "  \n",
        "\n",
        "First 5 rows and 10 columns of the train tfidf matrix:  \n",
        "[[ 0.1029514  -0.00972255  0.1807258  -0.02345218 -0.01122584]  \n",
        " [ 0.15955492  0.01012329  0.21545029 -0.00057363  0.11649812]  \n",
        " [-0.22639619  0.24005397  0.10130514  0.04625234  0.02976975]  \n",
        " [ 0.14726     0.088411    0.28386    -0.27970001 -0.32497999]  \n",
        " [ 0.11210106  0.00506607  0.23441343 -0.04273045  0.1871462 ]]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q9PE1ULelsl",
        "outputId": "d4ec4cc9-04af-4cd5-a375-48fbd71e3eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: glove_train_avg.csv, glove_test_avg.csv, glove_train_tfidf.csv, glove_test_tfidf.csv\n"
          ]
        }
      ],
      "source": [
        "# 6 Build tidy DataFrames from the four matrices returned earlier\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "df_train_glove_avg   = pd.DataFrame(X_train_glove_avg,\n",
        "                                    columns=[f\"dim_{i}\" for i in range(EMBED_DIM)])\n",
        "df_test_glove_avg    = pd.DataFrame(X_test_glove_avg,\n",
        "                                    columns=[f\"dim_{i}\" for i in range(EMBED_DIM)])\n",
        "\n",
        "df_train_glove_tfidf = pd.DataFrame(X_train_glove_tfidf,\n",
        "                                    columns=[f\"dim_{i}\" for i in range(EMBED_DIM)])\n",
        "df_test_glove_tfidf  = pd.DataFrame(X_test_glove_tfidf,\n",
        "                                    columns=[f\"dim_{i}\" for i in range(EMBED_DIM)])\n",
        "\n",
        "\n",
        "# 7 Optional: save to disk (one file per split/weighting scheme)\n",
        "# ---------------------------------------------------------------\n",
        "df_train_glove_avg.to_csv(\"glove_train_avg.csv\",   index=False)\n",
        "df_test_glove_avg.to_csv(\"glove_test_avg.csv\",    index=False)\n",
        "\n",
        "df_train_glove_tfidf.to_csv(\"glove_train_tfidf.csv\", index=False)\n",
        "df_test_glove_tfidf.to_csv(\"glove_test_tfidf.csv\",  index=False)\n",
        "\n",
        "print(\"Saved: glove_train_avg.csv, glove_test_avg.csv, \"\n",
        "      \"glove_train_tfidf.csv, glove_test_tfidf.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLm4pfLHelsl"
      },
      "source": [
        "##### Old\n",
        "- Used the full data['Article'] everywhere  \n",
        "- Hard-wired winsorisation inside main loop  \n",
        "- Separate code blocks for avg vs tf-idf  \n",
        "- Generated just one matrix  \n",
        "- Globals sprinkled in notebook\n",
        "  \n",
        "##### New  \n",
        "- Fits vocab & IDF only on X_train → avoids test leakage  \n",
        "- Extracted to fit_vocab_and_idf (re-usable & test-safe)  \n",
        "- Unified via embed_doc(kind=\"avg\"/\"tfidf\")  \n",
        "- Now returns four: train/test × (avg, tf-idf)  \n",
        "- Functions make the pipeline callable from anywhere  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yale_2025_qf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}